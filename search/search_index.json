{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome! \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. We are covering recipes for observability (o11y) solutions at AWS on this site. This includes managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana as well as agents, for example OpenTelemetry and Fluent Bit . We want to address the needs of both developers and infrastructure folks. The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specific solution: dimension examples destinations Prometheus \u00b7 Grafana \u00b7 OpenSearch \u00b7 CloudWatch \u00b7 Jaeger agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust infra & databases RDS \u00b7 DynamoDB \u00b7 MSK compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda \u00b7 AppRunner compute engine Fargate \u00b7 EC2 \u00b7 Lightsail For example, you might be looking for a solution to: Exemplary solution specification I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption Destination : S3 bucket for further consumption Agent : FluentBit Language : Python Infra & DB : N/A Compute unit : Kubernetes (EKS) Compute engine : EC2 Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes. To simplify navigation, we're grouping the six dimension into the following categories: By Compute : covering compute engines and units By Infra & Data : covering infrastructure and databases By Language : covering languages By Destination : covering telemetry and analytics Tasks : covering anomaly detection, alerting, troubleshooting, and more Learn more about dimensions \u2026 How to use \u00b6 You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, By Compute -> EKS -> Fargate -> Logs . Alternatively, you can search the site pressing / or the s key: License All recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution. How to contribute \u00b6 Start a discussion on what you plan to do and we take it from there. Learn more \u00b6 The recipes on this site are a good practices collection. In addition, there are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so check out: observability @ aws , a playlist of AWS folks talking about their projects and services. AWS observability workshops , to try out the offerings in a structured manner. The AWS monitoring and observability homepage with pointers to case studies and partners.","title":"Welcome!"},{"location":"#welcome","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. We are covering recipes for observability (o11y) solutions at AWS on this site. This includes managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana as well as agents, for example OpenTelemetry and Fluent Bit . We want to address the needs of both developers and infrastructure folks. The way we think about the o11y space is as follows: we decompose it into six dimensions you can then combine to arrive at a specific solution: dimension examples destinations Prometheus \u00b7 Grafana \u00b7 OpenSearch \u00b7 CloudWatch \u00b7 Jaeger agents ADOT \u00b7 Fluent Bit \u00b7 CW agent \u00b7 X-Ray agent languages Java \u00b7 Python \u00b7 .NET \u00b7 JavaScript \u00b7 Go \u00b7 Rust infra & databases RDS \u00b7 DynamoDB \u00b7 MSK compute unit Batch \u00b7 ECS \u00b7 EKS \u00b7 AEB \u00b7 Lambda \u00b7 AppRunner compute engine Fargate \u00b7 EC2 \u00b7 Lightsail For example, you might be looking for a solution to: Exemplary solution specification I need a logging solution for a Python app I'm running on EKS on Fargate with the goal to store the logs in an S3 bucket for further consumption Destination : S3 bucket for further consumption Agent : FluentBit Language : Python Infra & DB : N/A Compute unit : Kubernetes (EKS) Compute engine : EC2 Not every dimension needs to be specified and sometimes it's hard to decide where to start. Try different paths and compare the pros and cons of certain recipes. To simplify navigation, we're grouping the six dimension into the following categories: By Compute : covering compute engines and units By Infra & Data : covering infrastructure and databases By Language : covering languages By Destination : covering telemetry and analytics Tasks : covering anomaly detection, alerting, troubleshooting, and more Learn more about dimensions \u2026","title":"Welcome!"},{"location":"#how-to-use","text":"You can either use the top navigation menu to browse to a specific index page, starting with a rough selection. For example, By Compute -> EKS -> Fargate -> Logs . Alternatively, you can search the site pressing / or the s key: License All recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution.","title":"How to use"},{"location":"#how-to-contribute","text":"Start a discussion on what you plan to do and we take it from there.","title":"How to contribute"},{"location":"#learn-more","text":"The recipes on this site are a good practices collection. In addition, there are a number of places where you can learn more about the status of open source projects we use as well as about the managed services from the recipes, so check out: observability @ aws , a playlist of AWS folks talking about their projects and services. AWS observability workshops , to try out the offerings in a structured manner. The AWS monitoring and observability homepage with pointers to case studies and partners.","title":"Learn more"},{"location":"about/","text":"About \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. The content on this site is maintained by AWS open source observability service team members. Our goal is to improve the discovery of relevant good practices on how to set up and use AWS managed services and open source projects in the observability space. Recipes and content contributions in general so far are from the following people: Alolita Sharma Dieter Adant Michael Hausenblas Sheetal Joshi Vikram Venkataraman Imaya Kumar Jagannathan Note that all recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution. If you have any questions or suggestions, please use the discussion feature on the GitHub repo hosting the content of this site.","title":"About"},{"location":"about/#about","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. The content on this site is maintained by AWS open source observability service team members. Our goal is to improve the discovery of relevant good practices on how to set up and use AWS managed services and open source projects in the observability space. Recipes and content contributions in general so far are from the following people: Alolita Sharma Dieter Adant Michael Hausenblas Sheetal Joshi Vikram Venkataraman Imaya Kumar Jagannathan Note that all recipes published on this site are available via the MIT-0 license, a modification to the usual MIT license that removes the requirement for attribution. If you have any questions or suggestions, please use the discussion feature on the GitHub repo hosting the content of this site.","title":"About"},{"location":"aes/","text":"Amazon OpenSearch Service \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon OpenSearch Service (AOS), successor to Amazon Elasticsearch Service, makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), and visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Check out the following recipes: AOS tutorial: a quick start guide Get started with AOS: T-shirt-size your domain Getting started with AOS Log Analytics with AOS Getting started with Open Distro for Elasticsearch Know your data with Machine Learning Send CloudTrail Logs to AOS Searching DynamoDB Data with AOS Getting Started with Trace Analytics in AOS","title":"Amazon OpenSearch Service"},{"location":"aes/#amazon-opensearch-service","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon OpenSearch Service (AOS), successor to Amazon Elasticsearch Service, makes it easy for you to perform interactive log analytics, real-time application monitoring, website search, and more. OpenSearch is an open source, distributed search and analytics suite derived from Elasticsearch. It offers the latest versions of OpenSearch, support for 19 versions of Elasticsearch (1.5 to 7.10 versions), and visualization capabilities powered by OpenSearch Dashboards and Kibana (1.5 to 7.10 versions). Check out the following recipes: AOS tutorial: a quick start guide Get started with AOS: T-shirt-size your domain Getting started with AOS Log Analytics with AOS Getting started with Open Distro for Elasticsearch Know your data with Machine Learning Send CloudTrail Logs to AOS Searching DynamoDB Data with AOS Getting Started with Trace Analytics in AOS","title":"Amazon OpenSearch Service"},{"location":"alerting/","text":"Alerting \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section has a selection of recipes for various alerting systems and scenarios. Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS","title":"Alerting"},{"location":"alerting/#alerting","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section has a selection of recipes for various alerting systems and scenarios. Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS","title":"Alerting"},{"location":"amg/","text":"Amazon Managed Grafana \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Grafana is a fully managed service based on open source Grafana, enabling you to analyze your metrics, logs, and traces without having to provision servers, configure and update software, or do the heavy lifting involved in securing and scaling Grafana in production. You can create, explore, and share observability dashboards with your team, connecting to multiple data sources. Check out the following recipes: Basics \u00b6 Getting Started Using Terraform for automation Authentication and Access Control \u00b6 Direct SAML integration with identity providers Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO Integrating Google authentication via SAMLv2 Setting up Amazon Managed Grafana cross-account data source using customer managed IAM roles Fine-grained access control in Amazon Managed Grafana using Grafana Teams Data sources and Visualizations \u00b6 Using Athena in Amazon Managed Grafana Using Redshift in Amazon Managed Grafana Viewing custom metrics from statsd with Amazon Managed Service for Prometheus and Amazon Managed Grafana Setting up cross-account data source using customer managed IAM roles Others \u00b6 Monitoring hybrid environments Managing Grafana and Loki in a regulated multitenant environment Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana Workshop for Getting Started","title":"Amazon Managed Grafana"},{"location":"amg/#amazon-managed-grafana","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Grafana is a fully managed service based on open source Grafana, enabling you to analyze your metrics, logs, and traces without having to provision servers, configure and update software, or do the heavy lifting involved in securing and scaling Grafana in production. You can create, explore, and share observability dashboards with your team, connecting to multiple data sources. Check out the following recipes:","title":"Amazon Managed Grafana"},{"location":"amg/#basics","text":"Getting Started Using Terraform for automation","title":"Basics"},{"location":"amg/#authentication-and-access-control","text":"Direct SAML integration with identity providers Integrating identity providers (OneLogin, Ping Identity, Okta, and Azure AD) to SSO Integrating Google authentication via SAMLv2 Setting up Amazon Managed Grafana cross-account data source using customer managed IAM roles Fine-grained access control in Amazon Managed Grafana using Grafana Teams","title":"Authentication and Access Control"},{"location":"amg/#data-sources-and-visualizations","text":"Using Athena in Amazon Managed Grafana Using Redshift in Amazon Managed Grafana Viewing custom metrics from statsd with Amazon Managed Service for Prometheus and Amazon Managed Grafana Setting up cross-account data source using customer managed IAM roles","title":"Data sources and Visualizations"},{"location":"amg/#others","text":"Monitoring hybrid environments Managing Grafana and Loki in a regulated multitenant environment Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana Workshop for Getting Started","title":"Others"},{"location":"amp/","text":"Amazon Managed Service for Prometheus \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Service for Prometheus (AMP) is a Prometheus-compatible monitoring service that makes it easy to monitor containerized applications at scale. With AMP, you can use the Prometheus query language (PromQL) to monitor the performance of containerized workloads without having to manage the underlying infrastructure required to manage the ingestion, storage, and querying of operational metrics. Check out the following recipes: Getting Started with AMP Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Setting up cross-account ingestion into AMP Metrics collection from ECS using AMP Configuring Grafana Cloud Agent for AMP Set up cross-region metrics collection for AMP workspaces Best practices for migrating self-hosted Prometheus on EKS to AMP Workshop for Getting Started with AMP Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana Introducing Amazon EKS Observability Accelerator Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager","title":"Amazon Managed Service for Prometheus"},{"location":"amp/#amazon-managed-service-for-prometheus","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Service for Prometheus (AMP) is a Prometheus-compatible monitoring service that makes it easy to monitor containerized applications at scale. With AMP, you can use the Prometheus query language (PromQL) to monitor the performance of containerized workloads without having to manage the underlying infrastructure required to manage the ingestion, storage, and querying of operational metrics. Check out the following recipes: Getting Started with AMP Using ADOT in EKS on EC2 to ingest to AMP and visualize in AMG Setting up cross-account ingestion into AMP Metrics collection from ECS using AMP Configuring Grafana Cloud Agent for AMP Set up cross-region metrics collection for AMP workspaces Best practices for migrating self-hosted Prometheus on EKS to AMP Workshop for Getting Started with AMP Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana Introducing Amazon EKS Observability Accelerator Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager","title":"Amazon Managed Service for Prometheus"},{"location":"anomaly-detection/","text":"Anomaly Detection \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section contains recipes for anomaly detection. Enabling Anomaly Detection for a CloudWatch Metric","title":"Anomaly Detection"},{"location":"anomaly-detection/#anomaly-detection","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section contains recipes for anomaly detection. Enabling Anomaly Detection for a CloudWatch Metric","title":"Anomaly Detection"},{"location":"apprunner/","text":"AWS App Runner \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner builds and deploys the web application automatically, load balances traffic with encryption, scales to meet your traffic needs, and makes it easy for your services to communicate with other AWS services and applications that run in a private Amazon VPC. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications. Check out the following recipes: General \u00b6 Container Day - Docker Con | How Developers can get to production web applications at scale easily AWS Blog | Centralized observability for AWS App Runner services AWS Blog | Observability for AWS App Runner VPC networking AWS Blog | Controlling and monitoring AWS App Runner applications with Amazon EventBridge Logs \u00b6 Viewing App Runner logs streamed to CloudWatch Logs Metrics \u00b6 Viewing App Runner service metrics reported to CloudWatch Traces \u00b6 Getting Started with AWS X-Ray tracing for App Runner using AWS Distro for OpenTelemetry Containers from the Couch | AWS App Runner X-Ray Integration AWS Blog | Tracing an AWS App Runner service using AWS X-Ray with OpenTelemetry AWS Blog | Enabling AWS X-Ray tracing for AWS App Runner service using AWS Copilot CLI","title":"AWS App Runner"},{"location":"apprunner/#aws-app-runner","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. AWS App Runner is a fully managed service that makes it easy for developers to quickly deploy containerized web applications and APIs, at scale and with no prior infrastructure experience required. Start with your source code or a container image. App Runner builds and deploys the web application automatically, load balances traffic with encryption, scales to meet your traffic needs, and makes it easy for your services to communicate with other AWS services and applications that run in a private Amazon VPC. With App Runner, rather than thinking about servers or scaling, you have more time to focus on your applications. Check out the following recipes:","title":"AWS App Runner"},{"location":"apprunner/#general","text":"Container Day - Docker Con | How Developers can get to production web applications at scale easily AWS Blog | Centralized observability for AWS App Runner services AWS Blog | Observability for AWS App Runner VPC networking AWS Blog | Controlling and monitoring AWS App Runner applications with Amazon EventBridge","title":"General"},{"location":"apprunner/#logs","text":"Viewing App Runner logs streamed to CloudWatch Logs","title":"Logs"},{"location":"apprunner/#metrics","text":"Viewing App Runner service metrics reported to CloudWatch","title":"Metrics"},{"location":"apprunner/#traces","text":"Getting Started with AWS X-Ray tracing for App Runner using AWS Distro for OpenTelemetry Containers from the Couch | AWS App Runner X-Ray Integration AWS Blog | Tracing an AWS App Runner service using AWS X-Ray with OpenTelemetry AWS Blog | Enabling AWS X-Ray tracing for AWS App Runner service using AWS Copilot CLI","title":"Traces"},{"location":"cw/","text":"Amazon CloudWatch \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon CloudWatch (CW) is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. Check out the following recipes: Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS Implementing CloudWatch-centric observability for Kubernetes-native developers in EKS Create Canaries via CW Synthetics Cloudwatch Logs Insights for Quering Logs Lambda Insights Anomaly Detection via CloudWatch Metrics Alarms via CloudWatch Choosing container logging options to avoid backpressure Introducing CloudWatch Container Insights Prometheus Support with AWS Distro for OpenTelemetry on ECS and EKS Monitoring ECS containerized Applications and Microservices using CW Container Insights Monitoring EKS containerized Applications and Microservices using CW Container Insights Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags","title":"Amazon CloudWatch"},{"location":"cw/#amazon-cloudwatch","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon CloudWatch (CW) is a monitoring and observability service built for DevOps engineers, developers, site reliability engineers (SREs), and IT managers. CloudWatch collects monitoring and operational data in the form of logs, metrics, and events, providing you with a unified view of AWS resources, applications, and services that run on AWS and on-premises servers. Check out the following recipes: Build proactive database monitoring for RDS with CW Logs, Lambda, and SNS Implementing CloudWatch-centric observability for Kubernetes-native developers in EKS Create Canaries via CW Synthetics Cloudwatch Logs Insights for Quering Logs Lambda Insights Anomaly Detection via CloudWatch Metrics Alarms via CloudWatch Choosing container logging options to avoid backpressure Introducing CloudWatch Container Insights Prometheus Support with AWS Distro for OpenTelemetry on ECS and EKS Monitoring ECS containerized Applications and Microservices using CW Container Insights Monitoring EKS containerized Applications and Microservices using CW Container Insights Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags","title":"Amazon CloudWatch"},{"location":"dimensions/","text":"Dimensions \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions. What is a signal? When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply. Let's now have a look at each of the six dimensions one by one: Destinations \u00b6 In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure. Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations. So, how do the signals arrive in the destinations? Glad you asked, it's \u2026 Agents \u00b6 How the signals are collected and routed to analytics. The signals can come from two sources: either your application source code (see also the language section) or from things your application depends on, such as state managed in datastores as well as infrastructure like VPCs (see also the infra & data section). Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases. Languages \u00b6 This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation . You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics. Infrastructure & databases \u00b6 With this dimension we mean any sort of application-external dependencies, be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. Commonalities One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box. This dimension includes but is not limited to: AWS infrastructure, for example VPC flow logs . Secondary APIs such as Kubernetes control plane logs . Signals from datastores, such as or S3 , RDS or SQS . Compute unit \u00b6 The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes. Compute engine \u00b6 This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.","title":"Dimensions"},{"location":"dimensions/#dimensions","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In the context of this site we consider the o11y space along six dimensions. Looking at each dimension independently is beneficial from an synthetic point-of-view, that is, when you're trying to build out a concrete o11y solution for a given workload, spanning developer-related aspects such as the programming language used as well as operational topics, for example the runtime environment like containers or Lambda functions. What is a signal? When we say signal here we mean any kinds of o11y data and metadata points, including log entries, metrics, and traces. Unless we want to or have to be more specific, we use \"signal\" and it should be clear from the context what restrictions may apply. Let's now have a look at each of the six dimensions one by one:","title":"Dimensions"},{"location":"dimensions/#destinations","text":"In this dimension we consider all kinds of signal destinations including long term storage and graphical interfaces that let you consume signals. As a developer, you want access to an UI or an API that allows you to discover, look up, and correlate signals to troubleshoot your service. In an infrastructure or platform role you want access to an UI or an API that allows you to manage, discover, look up, and correlate signals to understand the state of the infrastructure. Ultimately, this is the most interesting dimension from a human point of view. However, in order to be able to reap the benefits we first have to invest a bit of work: we need to instrument our software and external dependencies and ingest the signals into the destinations. So, how do the signals arrive in the destinations? Glad you asked, it's \u2026","title":"Destinations"},{"location":"dimensions/#agents","text":"How the signals are collected and routed to analytics. The signals can come from two sources: either your application source code (see also the language section) or from things your application depends on, such as state managed in datastores as well as infrastructure like VPCs (see also the infra & data section). Agents are part of the telemetry that you would use to collect and ingest signals. The other part are the instrumented applications and infra pieces like databases.","title":"Agents"},{"location":"dimensions/#languages","text":"This dimension is concerned with the programming language you use for writing your service or application. Here, we're dealing with SDKs and libraries, such as the X-Ray SDKs or what OpenTelemetry provides in the context of instrumentation . You want to make sure that an o11y solution supports your programming language of choice for a given signal type such as logs or metrics.","title":"Languages"},{"location":"dimensions/#infrastructure-databases","text":"With this dimension we mean any sort of application-external dependencies, be it infrastructure like the VPC the service is running in or a datastore like RDS or DynamoDB or a queue like SQS. Commonalities One thing all the sources in this dimension have in common is that they are located outside of your application (as well as the compute environment your app runs in) and with that you have to treat them as an opaque box. This dimension includes but is not limited to: AWS infrastructure, for example VPC flow logs . Secondary APIs such as Kubernetes control plane logs . Signals from datastores, such as or S3 , RDS or SQS .","title":"Infrastructure &amp; databases"},{"location":"dimensions/#compute-unit","text":"The way your package, schedule, and run your code. For example, in Lambda that's a function and in ECS and EKS that unit is a container running in a tasks (ECS) or pods (EKS), respectively. Containerized environments like Kubernetes often allow for two options concerning telemetry deployments: as side cars or as per-node (instance) daemon processes.","title":"Compute unit"},{"location":"dimensions/#compute-engine","text":"This dimension refers to the base runtime environment, which may (in case of an EC2 instance, for example) or may not (serverless offerings such as Fargate or Lambda) be your responsibility to provision and patch. Depending on the compute engine you use, the telemetry part might already be part of the offering, for example, EKS on Fargate has log routing via Fluent Bit integrated.","title":"Compute engine"},{"location":"dynamodb/","text":"Amazon DynamoDB \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Check out the following recipes: Monitoring Amazon DynamoDB for operational awareness Searching DynamoDB data with Amazon Elasticsearch Service DynamoDB Contributor Insights","title":"Amazon DynamoDB"},{"location":"dynamodb/#amazon-dynamodb","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. Check out the following recipes: Monitoring Amazon DynamoDB for operational awareness Searching DynamoDB data with Amazon Elasticsearch Service DynamoDB Contributor Insights","title":"Amazon DynamoDB"},{"location":"ecs/","text":"Amazon Elastic Container Service \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications, deeply integrating with the rest of AWS. Check out the following recipes, grouped by compute engine: General \u00b6 Deployment patterns for the AWS Distro for OpenTelemetry Collector with ECS Simplifying Amazon ECS monitoring set up with AWS Distro for OpenTelemetry ECS on EC2 \u00b6 Logs \u00b6 Under the hood: FireLens for Amazon ECS Tasks Metrics \u00b6 Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS Metrics collection from ECS using Amazon Managed Service for Prometheus Sending Envoy metrics from AWS App Mesh to Amazon CloudWatch ECS on Fargate \u00b6 Logs \u00b6 Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit","title":"Amazon Elastic Container Service"},{"location":"ecs/#amazon-elastic-container-service","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Elastic Container Service (ECS) is a fully managed container orchestration service that helps you easily deploy, manage, and scale containerized applications, deeply integrating with the rest of AWS. Check out the following recipes, grouped by compute engine:","title":"Amazon Elastic Container Service"},{"location":"ecs/#general","text":"Deployment patterns for the AWS Distro for OpenTelemetry Collector with ECS Simplifying Amazon ECS monitoring set up with AWS Distro for OpenTelemetry","title":"General"},{"location":"ecs/#ecs-on-ec2","text":"","title":"ECS on EC2"},{"location":"ecs/#logs","text":"Under the hood: FireLens for Amazon ECS Tasks","title":"Logs"},{"location":"ecs/#metrics","text":"Using AWS Distro for OpenTelemetry collector for cross-account metrics collection on Amazon ECS Metrics collection from ECS using Amazon Managed Service for Prometheus Sending Envoy metrics from AWS App Mesh to Amazon CloudWatch","title":"Metrics"},{"location":"ecs/#ecs-on-fargate","text":"","title":"ECS on Fargate"},{"location":"ecs/#logs_1","text":"Sample logging architectures for FireLens on Amazon ECS and AWS Fargate using Fluent Bit","title":"Logs"},{"location":"eks/","text":"Amazon Elastic Kubernetes Service \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Elastic Kubernetes Service (EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS Cloud or on-premises. Check out the following recipes, grouped by compute engine: EKS on EC2 \u00b6 Logs \u00b6 Fluent Bit Integration in CloudWatch Container Insights for EKS Logging with EFK Stack Sample logging architectures for Fluent Bit and FluentD on EKS Metrics \u00b6 Getting Started with Amazon Managed Service for Prometheus Using ADOT in EKS on EC2 to ingest metrics to AMP and visualize in AMG Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus Monitoring cluster using Prometheus and Grafana Monitoring with Managed Prometheus and Managed Grafana CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana Traces \u00b6 Migrating X-Ray tracing to AWS Distro for OpenTelemetry Tracing with X-Ray EKS on Fargate \u00b6 Logs \u00b6 Fluent Bit for Amazon EKS on AWS Fargate is here Sample logging architectures for Fluent Bit and FluentD on EKS Metrics \u00b6 Using ADOT in EKS on Fargate to ingest metrics to AMP and visualize in AMG CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Traces \u00b6 Using ADOT in EKS on Fargate with AWS X-Ray Tracing with X-Ray","title":"Amazon Elastic Kubernetes Service"},{"location":"eks/#amazon-elastic-kubernetes-service","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Elastic Kubernetes Service (EKS) gives you the flexibility to start, run, and scale Kubernetes applications in the AWS Cloud or on-premises. Check out the following recipes, grouped by compute engine:","title":"Amazon Elastic Kubernetes Service"},{"location":"eks/#eks-on-ec2","text":"","title":"EKS on EC2"},{"location":"eks/#logs","text":"Fluent Bit Integration in CloudWatch Container Insights for EKS Logging with EFK Stack Sample logging architectures for Fluent Bit and FluentD on EKS","title":"Logs"},{"location":"eks/#metrics","text":"Getting Started with Amazon Managed Service for Prometheus Using ADOT in EKS on EC2 to ingest metrics to AMP and visualize in AMG Configuring Grafana Cloud Agent for Amazon Managed Service for Prometheus Monitoring cluster using Prometheus and Grafana Monitoring with Managed Prometheus and Managed Grafana CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces Monitoring App Mesh environment on EKS using Amazon Managed Service for Prometheus Monitor Istio on EKS using Amazon Managed Prometheus and Amazon Managed Grafana Proactive autoscaling of Kubernetes workloads with KEDA and Amazon CloudWatch Monitoring Amazon EKS Anywhere using Amazon Managed Service for Prometheus and Amazon Managed Grafana","title":"Metrics"},{"location":"eks/#traces","text":"Migrating X-Ray tracing to AWS Distro for OpenTelemetry Tracing with X-Ray","title":"Traces"},{"location":"eks/#eks-on-fargate","text":"","title":"EKS on Fargate"},{"location":"eks/#logs_1","text":"Fluent Bit for Amazon EKS on AWS Fargate is here Sample logging architectures for Fluent Bit and FluentD on EKS","title":"Logs"},{"location":"eks/#metrics_1","text":"Using ADOT in EKS on Fargate to ingest metrics to AMP and visualize in AMG CloudWatch Container Insights Set up cross-region metrics collection for AMP workspaces","title":"Metrics"},{"location":"eks/#traces_1","text":"Using ADOT in EKS on Fargate with AWS X-Ray Tracing with X-Ray","title":"Traces"},{"location":"infra/","text":"Infrastructure & Databases \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Networking \u00b6 Monitor your Application Load Balancers Monitor your Network Load Balancers VPC Flow Logs VPC Flow logs analysis using Amazon Elasticsearch Service Compute \u00b6 Amazon EKS control plane logging AWS Lambda monitoring and observability Databases, storage and queues \u00b6 Amazon Relational Database Service Amazon DynamoDB Amazon Managed Streaming for Apache Kafka Logging and monitoring in Amazon S3 Amazon SQS and AWS X-Ray Others \u00b6 Prometheus exporters","title":"Infra"},{"location":"infra/#infrastructure-databases","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned.","title":"Infrastructure &amp; Databases"},{"location":"infra/#networking","text":"Monitor your Application Load Balancers Monitor your Network Load Balancers VPC Flow Logs VPC Flow logs analysis using Amazon Elasticsearch Service","title":"Networking"},{"location":"infra/#compute","text":"Amazon EKS control plane logging AWS Lambda monitoring and observability","title":"Compute"},{"location":"infra/#databases-storage-and-queues","text":"Amazon Relational Database Service Amazon DynamoDB Amazon Managed Streaming for Apache Kafka Logging and monitoring in Amazon S3 Amazon SQS and AWS X-Ray","title":"Databases, storage and queues"},{"location":"infra/#others","text":"Prometheus exporters","title":"Others"},{"location":"java/","text":"Java \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. StatsD and Java Support in AWS Distro for OpenTelemetry","title":"Java"},{"location":"java/#java","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. StatsD and Java Support in AWS Distro for OpenTelemetry","title":"Java"},{"location":"lambda/","text":"AWS Lambda \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. Check out the following recipes: Logs \u00b6 Deploy and Monitor a Serverless Application Metrics \u00b6 Introducing CloudWatch Lambda Insights Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus Traces \u00b6 Auto-instrumenting a Python application with an AWS Distro for OpenTelemetry Lambda layer Tracing AWS Lambda functions in AWS X-Ray with OpenTelemetry","title":"AWS Lambda"},{"location":"lambda/#aws-lambda","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers, creating workload-aware cluster scaling logic, maintaining event integrations, or managing runtimes. Check out the following recipes:","title":"AWS Lambda"},{"location":"lambda/#logs","text":"Deploy and Monitor a Serverless Application","title":"Logs"},{"location":"lambda/#metrics","text":"Introducing CloudWatch Lambda Insights Exporting Cloudwatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus","title":"Metrics"},{"location":"lambda/#traces","text":"Auto-instrumenting a Python application with an AWS Distro for OpenTelemetry Lambda layer Tracing AWS Lambda functions in AWS X-Ray with OpenTelemetry","title":"Traces"},{"location":"msk/","text":"Amazon Managed Streaming for Apache Kafka \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition, Amazon MSK secures your Apache Kafka cluster by encrypting data at rest. Check out the following recipes: Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus","title":"Amazon Managed Streaming for Apache Kafka"},{"location":"msk/#amazon-managed-streaming-for-apache-kafka","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Managed Streaming for Apache Kafka (MSK) is a fully managed service that makes it easy for you to build and run applications that use Apache Kafka to process streaming data. Amazon MSK continuously monitors cluster health and automatically replaces unhealthy nodes with no downtime to your application. In addition, Amazon MSK secures your Apache Kafka cluster by encrypting data at rest. Check out the following recipes: Amazon Managed Streaming for Apache Kafka: Open Monitoring with Prometheus","title":"Amazon Managed Streaming for Apache Kafka"},{"location":"nodejs/","text":"Node.js \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. NodeJS library to generate embedded CloudWatch metrics","title":"Node.js"},{"location":"nodejs/#nodejs","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. NodeJS library to generate embedded CloudWatch metrics","title":"Node.js"},{"location":"rds/","text":"Amazon Relational Database Service \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Relational Database Service (RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Check out the following recipes: Build proactive database monitoring for RDS with CloudWatch Logs, Lambda, and SNS Monitor RDS for PostgreSQL and Aurora for PostgreSQL database log errors and set up notifications using CloudWatch Logging and monitoring in Amazon RDS Performance Insights metrics published to CloudWatch","title":"Amazon Relational Database Service"},{"location":"rds/#amazon-relational-database-service","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Amazon Relational Database Service (RDS) makes it easy to set up, operate, and scale a relational database in the cloud. It provides cost-efficient and resizable capacity while automating time-consuming administration tasks such as hardware provisioning, database setup, patching and backups. Check out the following recipes: Build proactive database monitoring for RDS with CloudWatch Logs, Lambda, and SNS Monitor RDS for PostgreSQL and Aurora for PostgreSQL database log errors and set up notifications using CloudWatch Logging and monitoring in Amazon RDS Performance Insights metrics published to CloudWatch","title":"Amazon Relational Database Service"},{"location":"telemetry/","text":"Telemetry \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed: Let's further dive into the concepts introduced in above figure. Sources \u00b6 We consider sources as something where signals come from. There are two types of sources: Things under your control, that is, the application source code, via instrumentation. Everything else you may use, such as managed services, not under your (direct) control. These types of sources are typically provided by AWS, exposing signals via an API. Agents \u00b6 In order to transpor signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals from the sources and, typically via configuration, determine where signals shoud go, optionally supporting filtering and aggregation. Agents? Routing? Shipping? Ingesting? There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly different things, we will use them here interchangeably. Canonically, we will refer to those intermediary transport components as agents. Destinations \u00b6 Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.","title":"Telemetry"},{"location":"telemetry/#telemetry","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Telemetry is all about how the signals are collected from various sources, including your own app and infrastructure and routed to destinations where they are consumed: Let's further dive into the concepts introduced in above figure.","title":"Telemetry"},{"location":"telemetry/#sources","text":"We consider sources as something where signals come from. There are two types of sources: Things under your control, that is, the application source code, via instrumentation. Everything else you may use, such as managed services, not under your (direct) control. These types of sources are typically provided by AWS, exposing signals via an API.","title":"Sources"},{"location":"telemetry/#agents","text":"In order to transpor signals from the sources to the destinations, you need some sort of intermediary we call agent. These agents receive or pull signals from the sources and, typically via configuration, determine where signals shoud go, optionally supporting filtering and aggregation. Agents? Routing? Shipping? Ingesting? There are many terms out there people use to refer to the process of getting the signals from sources to destinations including routing, shipping, aggregation, ingesting etc. and while they may mean slightly different things, we will use them here interchangeably. Canonically, we will refer to those intermediary transport components as agents.","title":"Agents"},{"location":"telemetry/#destinations","text":"Where signals end up, for consumption. No matter if you want to store signals for later consumption, if you want to dashboard them, set an alert if a certain condition is true, or correlate signals. All of those components that serve you as the end-user are destinations.","title":"Destinations"},{"location":"troubleshooting/","text":"Troubleshooting \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. We include troubleshooting recipes for various situations and dimensions in this section. Troubleshooting performance bottleneck in DynamoDB","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. We include troubleshooting recipes for various situations and dimensions in this section. Troubleshooting performance bottleneck in DynamoDB","title":"Troubleshooting"},{"location":"workshops/","text":"Workshops \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling. One Observability Workshop EKS Workshop ECS Workshop App Runner Workshop","title":"Workshops"},{"location":"workshops/#workshops","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. This section contains workshops to which you can return for samples and demonstrations around o11y systems and tooling. One Observability Workshop EKS Workshop ECS Workshop App Runner Workshop","title":"Workshops"},{"location":"recipes/amg-athena-plugin/","text":"Using Athena in Amazon Managed Grafana \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Amazon Athena \u2014a serverless, interactive query service allowing you to analyze data in Amazon S3 using standard SQL\u2014in Amazon Managed Grafana . This integration is enabled by the Athena data source for Grafana , an open source plugin available for you to use in any DIY Grafana instance as well as pre-installed in Amazon Managed Grafana. Note This guide will take approximately 20 minutes to complete. Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You have access to Amazon Athena from your account. Infrastructure \u00b6 Let's first set up the necessary infrastructure. Set up Amazon Athena \u00b6 We want to see how to use Athena in two different scenarios: one scenario around geographical data along with the Geomap plugin, and one in a security-relevant scenario around VPC flow logs. First, let's make sure Athena is set up and the datasets are loaded. Warning You have to use the Amazon Athena console to execute these queries. Grafana in general has read-only access to the data sources, so can not be used to create or update data. Load geographical data \u00b6 In this first use case we use a dataset from the Registry of Open Data on AWS . More specifically, we will use OpenStreetMap (OSM) to demonstrate the usage of the Athena plugin for a geographical data motivated use case. For that to work, we need to first get the OSM data into Athena. So, first off, create a new database in Athena. Go to the Athena console and there use the following three SQL queries to import the OSM data into the database. Query 1: CREATE EXTERNAL TABLE planet ( id BIGINT , type STRING , tags MAP < STRING , STRING > , lat DECIMAL ( 9 , 7 ), lon DECIMAL ( 10 , 7 ), nds ARRAY < STRUCT < ref : BIGINT >> , members ARRAY < STRUCT < type : STRING , ref : BIGINT , role : STRING >> , changeset BIGINT , timestamp TIMESTAMP , uid BIGINT , user STRING , version BIGINT ) STORED AS ORCFILE LOCATION 's3://osm-pds/planet/' ; Query 2: CREATE EXTERNAL TABLE planet_history ( id BIGINT , type STRING , tags MAP < STRING , STRING > , lat DECIMAL ( 9 , 7 ), lon DECIMAL ( 10 , 7 ), nds ARRAY < STRUCT < ref : BIGINT >> , members ARRAY < STRUCT < type : STRING , ref : BIGINT , role : STRING >> , changeset BIGINT , timestamp TIMESTAMP , uid BIGINT , user STRING , version BIGINT , visible BOOLEAN ) STORED AS ORCFILE LOCATION 's3://osm-pds/planet-history/' ; Query 3: CREATE EXTERNAL TABLE changesets ( id BIGINT , tags MAP < STRING , STRING > , created_at TIMESTAMP , open BOOLEAN , closed_at TIMESTAMP , comments_count BIGINT , min_lat DECIMAL ( 9 , 7 ), max_lat DECIMAL ( 9 , 7 ), min_lon DECIMAL ( 10 , 7 ), max_lon DECIMAL ( 10 , 7 ), num_changes BIGINT , uid BIGINT , user STRING ) STORED AS ORCFILE LOCATION 's3://osm-pds/changesets/' ; Load VPC flow logs data \u00b6 The second use case is a security-motivated one: analyzing network traffic using VPC Flow Logs . First, we need to tell EC2 to generate VPC Flow Logs for us. So, if you have not done this already, you go ahead now and create VPC flow logs either on the network interfaces level, subnet level, or VPC level. Note To improve query performance and minimize the storage footprint, we store the VPC flow logs in Parquet , a columnar storage format that supports nested data. For our setup it doesn't matter what option you choose (network interfaces, subnet, or VPC), as long as you publish them to an S3 bucket in Parquet format as shown below: Now, again via the Athena console , create the table for the VPC flow logs data in the same database you imported the OSM data, or create a new one, if you prefer to do so. Use the following SQL query and make sure that you're replacing VPC_FLOW_LOGS_LOCATION_IN_S3 with your own bucket/folder: CREATE EXTERNAL TABLE vpclogs ( ` version ` int , ` account_id ` string , ` interface_id ` string , ` srcaddr ` string , ` dstaddr ` string , ` srcport ` int , ` dstport ` int , ` protocol ` bigint , ` packets ` bigint , ` bytes ` bigint , ` start ` bigint , ` end ` bigint , ` action ` string , ` log_status ` string , ` vpc_id ` string , ` subnet_id ` string , ` instance_id ` string , ` tcp_flags ` int , ` type ` string , ` pkt_srcaddr ` string , ` pkt_dstaddr ` string , ` region ` string , ` az_id ` string , ` sublocation_type ` string , ` sublocation_id ` string , ` pkt_src_aws_service ` string , ` pkt_dst_aws_service ` string , ` flow_direction ` string , ` traffic_path ` int ) STORED AS PARQUET LOCATION 'VPC_FLOW_LOGS_LOCATION_IN_S3' For example, VPC_FLOW_LOGS_LOCATION_IN_S3 could look something like the following if you're using the S3 bucket allmyflowlogs : s3://allmyflowlogs/AWSLogs/12345678901/vpcflowlogs/eu-west-1/2021/ Now that the datasets are available in Athena, let's move on to Grafana. Set up Grafana \u00b6 We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace , for example by using the Getting Started guide, or use an existing one. Warning To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the IAM policies necessary to read the Athena resources. Further, note the following: The Athena workgroup you plan to use needs to be tagged with the key GrafanaDataSource and value true for the service managed permissions to be permitted to use the workgroup. The service-managed IAM policy only grants access to query result buckets that start with grafana-athena-query-results- , so for any other bucket you MUST add permissions manually. You have to add the s3:Get* and s3:List* permissions for the underlying data source being queried manually. To set up the Athena data source, use the left-hand toolbar and choose the lower AWS icon and then choose \"Athena\". Select your default region you want the plugin to discover the Athena data source to use, and then select the accounts that you want, and finally choose \"Add data source\". Alternatively, you can manually add and configure the Athena data source by following these steps: Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\". Search for \"Athena\". [OPTIONAL] Configure the authentication provider (recommended: workspace IAM role). Select your targeted Athena data source, database, and workgroup. If your workgroup doesn't have an output location configured already, specify the S3 bucket and folder to use for query results. Note that the bucket has to start with grafana-athena-query-results- if you want to benefit from the service-managed policy. Click \"Save & test\". You should see something like the following: Usage \u00b6 And now let's look at how to use our Athena datasets from Grafana. Use geographical data \u00b6 The OpenStreetMap (OSM) data in Athena can answer a number of questions, such as \"where are certain amenities\". Let's see that in action. For example, a SQL query against the OSM dataset to list places that offer food in the Las Vegas region is as follows: SELECT tags [ 'amenity' ] AS amenity , tags [ 'name' ] AS name , tags [ 'website' ] AS website , lat , lon FROM planet WHERE type = 'node' AND tags [ 'amenity' ] IN ( 'bar' , 'pub' , 'fast_food' , 'restaurant' ) AND lon BETWEEN - 115 . 5 AND - 114 . 5 AND lat BETWEEN 36 . 1 AND 36 . 3 LIMIT 500 ; Info The Las Vegas region in above query is defined as everything with a latitude between 36.1 and 36.3 as well as a longitude between -115.5 and -114.5 . You could turn that into a set of variables (one for each corner) and make the Geomap plugin adaptable to other regions. To visualize the OSM data using above query, you can import an example dashboard, available via osm-sample-dashboard.json that looks as follows: Note In above screen shot we use the Geomap visualization (in the left panel) to plot the data points. Use VPC flow logs data \u00b6 To analyze the VPC flow log data, detecting SSH and RDP traffic, use the following SQL queries. Getting a tabular overview on SSH/RDP traffic: SELECT srcaddr , dstaddr , account_id , action , protocol , bytes , log_status FROM vpclogs WHERE srcport in ( 22 , 3389 ) OR dstport IN ( 22 , 3389 ) ORDER BY start ASC ; Getting a time series view on bytes accepted and rejected: SELECT from_unixtime ( start ), sum ( bytes ), action FROM vpclogs WHERE srcport in ( 22 , 3389 ) OR dstport IN ( 22 , 3389 ) GROUP BY start , action ORDER BY start ASC ; Tip If you want to limit the amount of data queried in Athena, consider using the $__timeFilter macro. To visualize the VPC flow log data, you can import an example dashboard, available via vpcfl-sample-dashboard.json that looks as follows: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use Athena from Grafana! Cleanup \u00b6 Remove the OSM data from the Athena database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.","title":"Using Athena in Amazon Managed Grafana"},{"location":"recipes/amg-athena-plugin/#using-athena-in-amazon-managed-grafana","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Amazon Athena \u2014a serverless, interactive query service allowing you to analyze data in Amazon S3 using standard SQL\u2014in Amazon Managed Grafana . This integration is enabled by the Athena data source for Grafana , an open source plugin available for you to use in any DIY Grafana instance as well as pre-installed in Amazon Managed Grafana. Note This guide will take approximately 20 minutes to complete.","title":"Using Athena in Amazon Managed Grafana"},{"location":"recipes/amg-athena-plugin/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You have access to Amazon Athena from your account.","title":"Prerequisites"},{"location":"recipes/amg-athena-plugin/#infrastructure","text":"Let's first set up the necessary infrastructure.","title":"Infrastructure"},{"location":"recipes/amg-athena-plugin/#set-up-amazon-athena","text":"We want to see how to use Athena in two different scenarios: one scenario around geographical data along with the Geomap plugin, and one in a security-relevant scenario around VPC flow logs. First, let's make sure Athena is set up and the datasets are loaded. Warning You have to use the Amazon Athena console to execute these queries. Grafana in general has read-only access to the data sources, so can not be used to create or update data.","title":"Set up Amazon Athena"},{"location":"recipes/amg-athena-plugin/#load-geographical-data","text":"In this first use case we use a dataset from the Registry of Open Data on AWS . More specifically, we will use OpenStreetMap (OSM) to demonstrate the usage of the Athena plugin for a geographical data motivated use case. For that to work, we need to first get the OSM data into Athena. So, first off, create a new database in Athena. Go to the Athena console and there use the following three SQL queries to import the OSM data into the database. Query 1: CREATE EXTERNAL TABLE planet ( id BIGINT , type STRING , tags MAP < STRING , STRING > , lat DECIMAL ( 9 , 7 ), lon DECIMAL ( 10 , 7 ), nds ARRAY < STRUCT < ref : BIGINT >> , members ARRAY < STRUCT < type : STRING , ref : BIGINT , role : STRING >> , changeset BIGINT , timestamp TIMESTAMP , uid BIGINT , user STRING , version BIGINT ) STORED AS ORCFILE LOCATION 's3://osm-pds/planet/' ; Query 2: CREATE EXTERNAL TABLE planet_history ( id BIGINT , type STRING , tags MAP < STRING , STRING > , lat DECIMAL ( 9 , 7 ), lon DECIMAL ( 10 , 7 ), nds ARRAY < STRUCT < ref : BIGINT >> , members ARRAY < STRUCT < type : STRING , ref : BIGINT , role : STRING >> , changeset BIGINT , timestamp TIMESTAMP , uid BIGINT , user STRING , version BIGINT , visible BOOLEAN ) STORED AS ORCFILE LOCATION 's3://osm-pds/planet-history/' ; Query 3: CREATE EXTERNAL TABLE changesets ( id BIGINT , tags MAP < STRING , STRING > , created_at TIMESTAMP , open BOOLEAN , closed_at TIMESTAMP , comments_count BIGINT , min_lat DECIMAL ( 9 , 7 ), max_lat DECIMAL ( 9 , 7 ), min_lon DECIMAL ( 10 , 7 ), max_lon DECIMAL ( 10 , 7 ), num_changes BIGINT , uid BIGINT , user STRING ) STORED AS ORCFILE LOCATION 's3://osm-pds/changesets/' ;","title":"Load geographical data"},{"location":"recipes/amg-athena-plugin/#load-vpc-flow-logs-data","text":"The second use case is a security-motivated one: analyzing network traffic using VPC Flow Logs . First, we need to tell EC2 to generate VPC Flow Logs for us. So, if you have not done this already, you go ahead now and create VPC flow logs either on the network interfaces level, subnet level, or VPC level. Note To improve query performance and minimize the storage footprint, we store the VPC flow logs in Parquet , a columnar storage format that supports nested data. For our setup it doesn't matter what option you choose (network interfaces, subnet, or VPC), as long as you publish them to an S3 bucket in Parquet format as shown below: Now, again via the Athena console , create the table for the VPC flow logs data in the same database you imported the OSM data, or create a new one, if you prefer to do so. Use the following SQL query and make sure that you're replacing VPC_FLOW_LOGS_LOCATION_IN_S3 with your own bucket/folder: CREATE EXTERNAL TABLE vpclogs ( ` version ` int , ` account_id ` string , ` interface_id ` string , ` srcaddr ` string , ` dstaddr ` string , ` srcport ` int , ` dstport ` int , ` protocol ` bigint , ` packets ` bigint , ` bytes ` bigint , ` start ` bigint , ` end ` bigint , ` action ` string , ` log_status ` string , ` vpc_id ` string , ` subnet_id ` string , ` instance_id ` string , ` tcp_flags ` int , ` type ` string , ` pkt_srcaddr ` string , ` pkt_dstaddr ` string , ` region ` string , ` az_id ` string , ` sublocation_type ` string , ` sublocation_id ` string , ` pkt_src_aws_service ` string , ` pkt_dst_aws_service ` string , ` flow_direction ` string , ` traffic_path ` int ) STORED AS PARQUET LOCATION 'VPC_FLOW_LOGS_LOCATION_IN_S3' For example, VPC_FLOW_LOGS_LOCATION_IN_S3 could look something like the following if you're using the S3 bucket allmyflowlogs : s3://allmyflowlogs/AWSLogs/12345678901/vpcflowlogs/eu-west-1/2021/ Now that the datasets are available in Athena, let's move on to Grafana.","title":"Load VPC flow logs data"},{"location":"recipes/amg-athena-plugin/#set-up-grafana","text":"We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace , for example by using the Getting Started guide, or use an existing one. Warning To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the IAM policies necessary to read the Athena resources. Further, note the following: The Athena workgroup you plan to use needs to be tagged with the key GrafanaDataSource and value true for the service managed permissions to be permitted to use the workgroup. The service-managed IAM policy only grants access to query result buckets that start with grafana-athena-query-results- , so for any other bucket you MUST add permissions manually. You have to add the s3:Get* and s3:List* permissions for the underlying data source being queried manually. To set up the Athena data source, use the left-hand toolbar and choose the lower AWS icon and then choose \"Athena\". Select your default region you want the plugin to discover the Athena data source to use, and then select the accounts that you want, and finally choose \"Add data source\". Alternatively, you can manually add and configure the Athena data source by following these steps: Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\". Search for \"Athena\". [OPTIONAL] Configure the authentication provider (recommended: workspace IAM role). Select your targeted Athena data source, database, and workgroup. If your workgroup doesn't have an output location configured already, specify the S3 bucket and folder to use for query results. Note that the bucket has to start with grafana-athena-query-results- if you want to benefit from the service-managed policy. Click \"Save & test\". You should see something like the following:","title":"Set up Grafana"},{"location":"recipes/amg-athena-plugin/#usage","text":"And now let's look at how to use our Athena datasets from Grafana.","title":"Usage"},{"location":"recipes/amg-athena-plugin/#use-geographical-data","text":"The OpenStreetMap (OSM) data in Athena can answer a number of questions, such as \"where are certain amenities\". Let's see that in action. For example, a SQL query against the OSM dataset to list places that offer food in the Las Vegas region is as follows: SELECT tags [ 'amenity' ] AS amenity , tags [ 'name' ] AS name , tags [ 'website' ] AS website , lat , lon FROM planet WHERE type = 'node' AND tags [ 'amenity' ] IN ( 'bar' , 'pub' , 'fast_food' , 'restaurant' ) AND lon BETWEEN - 115 . 5 AND - 114 . 5 AND lat BETWEEN 36 . 1 AND 36 . 3 LIMIT 500 ; Info The Las Vegas region in above query is defined as everything with a latitude between 36.1 and 36.3 as well as a longitude between -115.5 and -114.5 . You could turn that into a set of variables (one for each corner) and make the Geomap plugin adaptable to other regions. To visualize the OSM data using above query, you can import an example dashboard, available via osm-sample-dashboard.json that looks as follows: Note In above screen shot we use the Geomap visualization (in the left panel) to plot the data points.","title":"Use geographical data"},{"location":"recipes/amg-athena-plugin/#use-vpc-flow-logs-data","text":"To analyze the VPC flow log data, detecting SSH and RDP traffic, use the following SQL queries. Getting a tabular overview on SSH/RDP traffic: SELECT srcaddr , dstaddr , account_id , action , protocol , bytes , log_status FROM vpclogs WHERE srcport in ( 22 , 3389 ) OR dstport IN ( 22 , 3389 ) ORDER BY start ASC ; Getting a time series view on bytes accepted and rejected: SELECT from_unixtime ( start ), sum ( bytes ), action FROM vpclogs WHERE srcport in ( 22 , 3389 ) OR dstport IN ( 22 , 3389 ) GROUP BY start , action ORDER BY start ASC ; Tip If you want to limit the amount of data queried in Athena, consider using the $__timeFilter macro. To visualize the VPC flow log data, you can import an example dashboard, available via vpcfl-sample-dashboard.json that looks as follows: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use Athena from Grafana!","title":"Use VPC flow logs data"},{"location":"recipes/amg-athena-plugin/#cleanup","text":"Remove the OSM data from the Athena database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/amg-automation-tf/","text":"Using Terraform for Amazon Managed Grafana automation \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how use Terraform to automate Amazon Managed Grafana , for example to add datasources or dashboards consistently across a number of workspaces. Note This guide will take approximately 30 minutes to complete. Prerequisites \u00b6 The AWS command line is installed and configured in your local environment. You have the Terraform command line installed in your local environment. You have an Amazon Managed Service for Prometheus workspace ready to use. You have an Amazon Managed Grafana workspace ready to use. Set up Amazon Managed Grafana \u00b6 In order for Terraform to authenticate against Grafana, we are using an API Key, which acts as a kind of password. Info The API key is an RFC 6750 HTTP Bearer header with a 51 character long alpha-numeric value authenticating the caller with every request against the Grafana API. So, before we can set up the Terraform manifest, we first need to create an API key. You do this via the Grafana UI as follows. First, select from the left-hand side menu in the Configuration section the API keys menu item: Now create a new API key, give it a name that makes sense for your task at hand, assign it Admin role and set the duration time to, for example, one day: Note The API key is valid for a limited time, in AMG you can use values up to 30 days. Once you hit the Add button you should see a pop-up dialog that contains the API key: Warning This is the only time you will see the API key, so store it from here in a safe place, we will need it in the Terraform manifest later. With this we've set up everything we need in Amazon Managed Grafana in order to use Terraform for automation, so let's move on to this step. Automation with Terraform \u00b6 Preparing Terraform \u00b6 For Terraform to be able to interact with Grafana, we're using the official Grafana provider in version 1.13.3 or above. In the following, we want to automate the creation of a data source, in our case we want to add a Prometheus data source , to be exact, an AMP workspace. First, create a file called main.tf with the following content: terraform { required_providers { grafana = { source = \"grafana/grafana\" version = \">= 1.13.3\" } } } provider \"grafana\" { url = \"INSERT YOUR GRAFANA WORKSPACE URL HERE\" auth = \"INSERT YOUR API KEY HERE\" } resource \"grafana_data_source\" \"prometheus\" { type = \"prometheus\" name = \"amp\" is_default = true url = \"INSERT YOUR AMP WORKSPACE URL HERE \" json_data { http_method = \"POST\" sigv4_auth = true sigv4_auth_type = \"workspace-iam-role\" sigv4_region = \"eu-west-1\" } } In above file you need to insert three values that depend on your environment. In the Grafana provider section: url \u2026 the Grafana workspace URL which looks something like the following: https://xxxxxxxx.grafana-workspace.eu-west-1.amazonaws.com . auth \u2026 the API key you have created in the previous step. In the Prometheus resource section, insert the url which is the AMP workspace URL in the form of https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx . Note If you're using Amazon Managed Grafana in a different region than the one shown in the file, you will have to, in addition to above, also set the sigv4_region to your region. To wrap up the preparation phase, let's now initialize Terraform: $ terraform init Initializing the backend... Initializing provider plugins... - Finding grafana/grafana versions matching \">= 1.13.3\"... - Installing grafana/grafana v1.13.3... - Installed grafana/grafana v1.13.3 (signed by a HashiCorp partner, key ID 570AA42029AE241A) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. With that, we're all set and can use Terraform to automate the data source creation as explained in the following. Using Terraform \u00b6 Usually, you would first have a look what Terraform's plan is, like so: $ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # grafana_data_source.prometheus will be created + resource \"grafana_data_source\" \"prometheus\" { + access_mode = \"proxy\" + basic_auth_enabled = false + id = (known after apply) + is_default = true + name = \"amp\" + type = \"prometheus\" + url = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxx/\" + json_data { + http_method = \"POST\" + sigv4_auth = true + sigv4_auth_type = \"workspace-iam-role\" + sigv4_region = \"eu-west-1\" } } Plan: 1 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. If you're happy with what you see there, you can apply the plan: $ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # grafana_data_source.prometheus will be created + resource \"grafana_data_source\" \"prometheus\" { + access_mode = \"proxy\" + basic_auth_enabled = false + id = (known after apply) + is_default = true + name = \"amp\" + type = \"prometheus\" + url = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx/\" + json_data { + http_method = \"POST\" + sigv4_auth = true + sigv4_auth_type = \"workspace-iam-role\" + sigv4_region = \"eu-west-1\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes grafana_data_source.prometheus: Creating... grafana_data_source.prometheus: Creation complete after 1s [id=10] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. When you now go to the data source list in Grafana you should see something like the following: To verify if your newly created data source works, you can hit the blue Save & test button at the bottom and you should see a Data source is working confirmation message as a result here. You can use Terraform also to automate other things, for example, the Grafana provider supports managing folders and dashboards. Let's say you want to create a folder to organize your dashboards, for example: resource \"grafana_folder\" \"examplefolder\" { title = \"devops\" } Further, say you have a dashboard called example-dashboard.json , and you want to create it in the folder from above, then you would use the following snippet: resource \"grafana_dashboard\" \"exampledashboard\" { folder = grafana_folder.examplefolder.id config_json = file(\"example-dashboard.json\") } Terraform is a powerful tool for automation and you can use it as shown here to manage your Grafana resources. Note Keep in mind, though, that the state in Terraform is, by default, managed locally. This means, if you plan to collaboratively work with Terraform, you need to pick one of the options available that allow you to share the state across a team. Cleanup \u00b6 Remove the Amazon Managed Grafana workspace by removing it from the console.","title":"Using Terraform for Amazon Managed Grafana automation"},{"location":"recipes/amg-automation-tf/#using-terraform-for-amazon-managed-grafana-automation","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how use Terraform to automate Amazon Managed Grafana , for example to add datasources or dashboards consistently across a number of workspaces. Note This guide will take approximately 30 minutes to complete.","title":"Using Terraform for Amazon Managed Grafana automation"},{"location":"recipes/amg-automation-tf/#prerequisites","text":"The AWS command line is installed and configured in your local environment. You have the Terraform command line installed in your local environment. You have an Amazon Managed Service for Prometheus workspace ready to use. You have an Amazon Managed Grafana workspace ready to use.","title":"Prerequisites"},{"location":"recipes/amg-automation-tf/#set-up-amazon-managed-grafana","text":"In order for Terraform to authenticate against Grafana, we are using an API Key, which acts as a kind of password. Info The API key is an RFC 6750 HTTP Bearer header with a 51 character long alpha-numeric value authenticating the caller with every request against the Grafana API. So, before we can set up the Terraform manifest, we first need to create an API key. You do this via the Grafana UI as follows. First, select from the left-hand side menu in the Configuration section the API keys menu item: Now create a new API key, give it a name that makes sense for your task at hand, assign it Admin role and set the duration time to, for example, one day: Note The API key is valid for a limited time, in AMG you can use values up to 30 days. Once you hit the Add button you should see a pop-up dialog that contains the API key: Warning This is the only time you will see the API key, so store it from here in a safe place, we will need it in the Terraform manifest later. With this we've set up everything we need in Amazon Managed Grafana in order to use Terraform for automation, so let's move on to this step.","title":"Set up Amazon Managed Grafana"},{"location":"recipes/amg-automation-tf/#automation-with-terraform","text":"","title":"Automation with Terraform"},{"location":"recipes/amg-automation-tf/#preparing-terraform","text":"For Terraform to be able to interact with Grafana, we're using the official Grafana provider in version 1.13.3 or above. In the following, we want to automate the creation of a data source, in our case we want to add a Prometheus data source , to be exact, an AMP workspace. First, create a file called main.tf with the following content: terraform { required_providers { grafana = { source = \"grafana/grafana\" version = \">= 1.13.3\" } } } provider \"grafana\" { url = \"INSERT YOUR GRAFANA WORKSPACE URL HERE\" auth = \"INSERT YOUR API KEY HERE\" } resource \"grafana_data_source\" \"prometheus\" { type = \"prometheus\" name = \"amp\" is_default = true url = \"INSERT YOUR AMP WORKSPACE URL HERE \" json_data { http_method = \"POST\" sigv4_auth = true sigv4_auth_type = \"workspace-iam-role\" sigv4_region = \"eu-west-1\" } } In above file you need to insert three values that depend on your environment. In the Grafana provider section: url \u2026 the Grafana workspace URL which looks something like the following: https://xxxxxxxx.grafana-workspace.eu-west-1.amazonaws.com . auth \u2026 the API key you have created in the previous step. In the Prometheus resource section, insert the url which is the AMP workspace URL in the form of https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx . Note If you're using Amazon Managed Grafana in a different region than the one shown in the file, you will have to, in addition to above, also set the sigv4_region to your region. To wrap up the preparation phase, let's now initialize Terraform: $ terraform init Initializing the backend... Initializing provider plugins... - Finding grafana/grafana versions matching \">= 1.13.3\"... - Installing grafana/grafana v1.13.3... - Installed grafana/grafana v1.13.3 (signed by a HashiCorp partner, key ID 570AA42029AE241A) Partner and community providers are signed by their developers. If you'd like to know more about provider signing, you can read about it here: https://www.terraform.io/docs/cli/plugins/signing.html Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run \"terraform init\" in the future. Terraform has been successfully initialized! You may now begin working with Terraform. Try running \"terraform plan\" to see any changes that are required for your infrastructure. All Terraform commands should now work. If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. With that, we're all set and can use Terraform to automate the data source creation as explained in the following.","title":"Preparing Terraform"},{"location":"recipes/amg-automation-tf/#using-terraform","text":"Usually, you would first have a look what Terraform's plan is, like so: $ terraform plan Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # grafana_data_source.prometheus will be created + resource \"grafana_data_source\" \"prometheus\" { + access_mode = \"proxy\" + basic_auth_enabled = false + id = (known after apply) + is_default = true + name = \"amp\" + type = \"prometheus\" + url = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxx/\" + json_data { + http_method = \"POST\" + sigv4_auth = true + sigv4_auth_type = \"workspace-iam-role\" + sigv4_region = \"eu-west-1\" } } Plan: 1 to add, 0 to change, 0 to destroy. \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Note: You didn't use the -out option to save this plan, so Terraform can't guarantee to take exactly these actions if you run \"terraform apply\" now. If you're happy with what you see there, you can apply the plan: $ terraform apply Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # grafana_data_source.prometheus will be created + resource \"grafana_data_source\" \"prometheus\" { + access_mode = \"proxy\" + basic_auth_enabled = false + id = (known after apply) + is_default = true + name = \"amp\" + type = \"prometheus\" + url = \"https://aps-workspaces.eu-west-1.amazonaws.com/workspaces/ws-xxxxxxxxx/\" + json_data { + http_method = \"POST\" + sigv4_auth = true + sigv4_auth_type = \"workspace-iam-role\" + sigv4_region = \"eu-west-1\" } } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes grafana_data_source.prometheus: Creating... grafana_data_source.prometheus: Creation complete after 1s [id=10] Apply complete! Resources: 1 added, 0 changed, 0 destroyed. When you now go to the data source list in Grafana you should see something like the following: To verify if your newly created data source works, you can hit the blue Save & test button at the bottom and you should see a Data source is working confirmation message as a result here. You can use Terraform also to automate other things, for example, the Grafana provider supports managing folders and dashboards. Let's say you want to create a folder to organize your dashboards, for example: resource \"grafana_folder\" \"examplefolder\" { title = \"devops\" } Further, say you have a dashboard called example-dashboard.json , and you want to create it in the folder from above, then you would use the following snippet: resource \"grafana_dashboard\" \"exampledashboard\" { folder = grafana_folder.examplefolder.id config_json = file(\"example-dashboard.json\") } Terraform is a powerful tool for automation and you can use it as shown here to manage your Grafana resources. Note Keep in mind, though, that the state in Terraform is, by default, managed locally. This means, if you plan to collaboratively work with Terraform, you need to pick one of the options available that allow you to share the state across a team.","title":"Using Terraform"},{"location":"recipes/amg-automation-tf/#cleanup","text":"Remove the Amazon Managed Grafana workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/amg-google-auth-saml/","text":"Configure Google Workspaces authentication with Amazon Managed Grafana using SAML \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this guide, we will walk through how you can setup Google Workspaces as an identity provider (IdP) for Amazon Managed Grafana using SAML v2.0 protocol. In order to follow this guide you need to create a paid Google Workspaces account in addition to having an Amazon Managed Grafana workspace created. Create Amazon Managed Grafana workspace \u00b6 Log into the Amazon Managed Grafana console and click Create workspace. In the following screen, provide a workspace name as shown below. Then click Next : In the Configure settings page, select Security Assertion Markup Language (SAML) option so you can configure a SAML based Identity Provider for users to log in: Select the data sources you want to choose and click Next : Click on Create workspace button in the Review and create screen: This will create a new Amazon Managed Grafana workspace as shown below: Configure Google Workspaces \u00b6 Login to Google Workspaces with Super Admin permissions and go to Web and mobile apps under Apps section. There, click on Add App and select Add custom SAML app. Now give the app a name as shown below. Click CONTINUE. : On the next screen, click on DOWNLOAD METADATA button to download the SAML metadata file. Click CONTINUE. On the next screen, you will see the ACS URL, Entity ID and Start URL fields. You can get the values for these fields from the Amazon Managed Grafana console. Select EMAIL from the drop down in the Name ID format field and select Basic Information > Primary email in the Name ID field. Click CONTINUE. In the Attribute mapping screen, make the mapping between Google Directory attributes and App attributes as shown in the screenshot below For users logging in through Google authentication to have Admin privileges in Amazon Managed Grafana , set the Department field\u2019s value as monitoring . You can choose any field and any value for this. Whatever you choose to use on the Google Workspaces side, make sure you make the mapping on Amazon Managed Grafana SAML settings to reflect that. Upload SAML metadata into Amazon Managed Grafana \u00b6 Now in the Amazon Managed Grafana console, click Upload or copy/paste option and select Choose file button to upload the SAML metadata file downloaded from Google Workspaces, earlier. In the Assertion mapping section, type in Department in the Assertion attribute role field and monitoring in the Admin role values field. This will allow users logging in with Department as monitoring to have Admin privileges in Grafana so they can perform administrator duties such as creating dashboards and datasources. Set values under Additional settings - optional section as shown in the screenshot below. Click on Save SAML configuration : Now Amazon Managed Grafana is set up to authenticate users using Google Workspaces. When users login, they will be redirected to the Google login page like so: After entering their credentials, they will be logged into Grafana as shown in the screenshot below. As you can see, the user was able to successfully login to Grafana using Google Workspaces authentication.","title":"Configure Google Workspaces authentication with Amazon Managed Grafana using SAML"},{"location":"recipes/amg-google-auth-saml/#configure-google-workspaces-authentication-with-amazon-managed-grafana-using-saml","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this guide, we will walk through how you can setup Google Workspaces as an identity provider (IdP) for Amazon Managed Grafana using SAML v2.0 protocol. In order to follow this guide you need to create a paid Google Workspaces account in addition to having an Amazon Managed Grafana workspace created.","title":"Configure Google Workspaces authentication with Amazon Managed Grafana using SAML"},{"location":"recipes/amg-google-auth-saml/#create-amazon-managed-grafana-workspace","text":"Log into the Amazon Managed Grafana console and click Create workspace. In the following screen, provide a workspace name as shown below. Then click Next : In the Configure settings page, select Security Assertion Markup Language (SAML) option so you can configure a SAML based Identity Provider for users to log in: Select the data sources you want to choose and click Next : Click on Create workspace button in the Review and create screen: This will create a new Amazon Managed Grafana workspace as shown below:","title":"Create Amazon Managed Grafana workspace"},{"location":"recipes/amg-google-auth-saml/#configure-google-workspaces","text":"Login to Google Workspaces with Super Admin permissions and go to Web and mobile apps under Apps section. There, click on Add App and select Add custom SAML app. Now give the app a name as shown below. Click CONTINUE. : On the next screen, click on DOWNLOAD METADATA button to download the SAML metadata file. Click CONTINUE. On the next screen, you will see the ACS URL, Entity ID and Start URL fields. You can get the values for these fields from the Amazon Managed Grafana console. Select EMAIL from the drop down in the Name ID format field and select Basic Information > Primary email in the Name ID field. Click CONTINUE. In the Attribute mapping screen, make the mapping between Google Directory attributes and App attributes as shown in the screenshot below For users logging in through Google authentication to have Admin privileges in Amazon Managed Grafana , set the Department field\u2019s value as monitoring . You can choose any field and any value for this. Whatever you choose to use on the Google Workspaces side, make sure you make the mapping on Amazon Managed Grafana SAML settings to reflect that.","title":"Configure Google Workspaces"},{"location":"recipes/amg-google-auth-saml/#upload-saml-metadata-into-amazon-managed-grafana","text":"Now in the Amazon Managed Grafana console, click Upload or copy/paste option and select Choose file button to upload the SAML metadata file downloaded from Google Workspaces, earlier. In the Assertion mapping section, type in Department in the Assertion attribute role field and monitoring in the Admin role values field. This will allow users logging in with Department as monitoring to have Admin privileges in Grafana so they can perform administrator duties such as creating dashboards and datasources. Set values under Additional settings - optional section as shown in the screenshot below. Click on Save SAML configuration : Now Amazon Managed Grafana is set up to authenticate users using Google Workspaces. When users login, they will be redirected to the Google login page like so: After entering their credentials, they will be logged into Grafana as shown in the screenshot below. As you can see, the user was able to successfully login to Grafana using Google Workspaces authentication.","title":"Upload SAML metadata into Amazon Managed Grafana"},{"location":"recipes/amg-redshift-plugin/","text":"Using Redshift in Amazon Managed Grafana \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Amazon Redshift \u2014a petabyte-scale data warehouse service using standard SQL\u2014in Amazon Managed Grafana . This integration is enabled by the Redshift data source for Grafana , an open source plugin available for you to use in any DIY Grafana instance as well as pre-installed in Amazon Managed Grafana. Note This guide will take approximately 10 minutes to complete. Prerequisites \u00b6 You have admin access to Amazon Redshift from your account. Tag your Amazon Redshift cluster with GrafanaDataSource: true . In order to benefit from the service-managed policies, create the database credentials in one of the following ways: If you want to use the default mechanism, that is, the temporary credentials option, to authenticate against the Redshift database, you must create a database user named redshift_data_api_user . If you want to use the credentials from Secrets Manager, you must tag the secret with RedshiftQueryOwner: true . Tip For more information on how to work with the service-managed or custom policies, see the examples in the Amazon Managed Grafana docs . Infrastructure \u00b6 We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace , for example by using the Getting Started guide, or use an existing one. Note To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the IAM policies necessary to read the Athena resources. To set up the Athena data source, use the left-hand toolbar and choose the lower AWS icon and then choose \"Redshift\". Select your default region you want the plugin to discover the Redshift data source to use, and then select the accounts that you want, and finally choose \"Add data source\". Alternatively, you can manually add and configure the Redshift data source by following these steps: Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\". Search for \"Redshift\". [OPTIONAL] Configure the authentication provider (recommended: workspace IAM role). Provide the \"Cluster Identifier\", \"Database\", and \"Database User\" values. Click \"Save & test\". You should see something like the following: Usage \u00b6 We will be using the Redshift Advance Monitoring setup. Since all is available out of the box, there's nothing else to configure at this point. You can import the Redshift monitoring dashboard, included in the Redshift plugin. Once imported you should see something like this: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use Redshift from Grafana! Cleanup \u00b6 Remove the Redshift database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.","title":"Using Redshift in Amazon Managed Grafana"},{"location":"recipes/amg-redshift-plugin/#using-redshift-in-amazon-managed-grafana","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Amazon Redshift \u2014a petabyte-scale data warehouse service using standard SQL\u2014in Amazon Managed Grafana . This integration is enabled by the Redshift data source for Grafana , an open source plugin available for you to use in any DIY Grafana instance as well as pre-installed in Amazon Managed Grafana. Note This guide will take approximately 10 minutes to complete.","title":"Using Redshift in Amazon Managed Grafana"},{"location":"recipes/amg-redshift-plugin/#prerequisites","text":"You have admin access to Amazon Redshift from your account. Tag your Amazon Redshift cluster with GrafanaDataSource: true . In order to benefit from the service-managed policies, create the database credentials in one of the following ways: If you want to use the default mechanism, that is, the temporary credentials option, to authenticate against the Redshift database, you must create a database user named redshift_data_api_user . If you want to use the credentials from Secrets Manager, you must tag the secret with RedshiftQueryOwner: true . Tip For more information on how to work with the service-managed or custom policies, see the examples in the Amazon Managed Grafana docs .","title":"Prerequisites"},{"location":"recipes/amg-redshift-plugin/#infrastructure","text":"We need a Grafana instance, so go ahead and set up a new Amazon Managed Grafana workspace , for example by using the Getting Started guide, or use an existing one. Note To use AWS data source configuration, first go to the Amazon Managed Grafana console to enable service-mananged IAM roles that grant the workspace the IAM policies necessary to read the Athena resources. To set up the Athena data source, use the left-hand toolbar and choose the lower AWS icon and then choose \"Redshift\". Select your default region you want the plugin to discover the Redshift data source to use, and then select the accounts that you want, and finally choose \"Add data source\". Alternatively, you can manually add and configure the Redshift data source by following these steps: Click on the \"Configurations\" icon on the left-hand toolbar and then on \"Add data source\". Search for \"Redshift\". [OPTIONAL] Configure the authentication provider (recommended: workspace IAM role). Provide the \"Cluster Identifier\", \"Database\", and \"Database User\" values. Click \"Save & test\". You should see something like the following:","title":"Infrastructure"},{"location":"recipes/amg-redshift-plugin/#usage","text":"We will be using the Redshift Advance Monitoring setup. Since all is available out of the box, there's nothing else to configure at this point. You can import the Redshift monitoring dashboard, included in the Redshift plugin. Once imported you should see something like this: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use Redshift from Grafana!","title":"Usage"},{"location":"recipes/amg-redshift-plugin/#cleanup","text":"Remove the Redshift database you've been using and then the Amazon Managed Grafana workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/amp-alertmanager-terraform/","text":"Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe, we will demonstrate how you can use Terraform to provision Amazon Managed Service for Prometheus and configure rules management and alert manager to send notification to a SNS topic if a certain condition is met. Note This guide will take approximately 30 minutes to complete. Prerequisites \u00b6 You will need the following to complete the setup: Amazon EKS cluster AWS CLI version 2 Terraform CLI AWS Distro for OpenTelemetry(ADOT) eksctl kubectl jq helm SNS topic awscurl In the recipe, we will use a sample application in order to demonstrate the metric scraping using ADOT and remote write the metrics to the Amazon Managed Service for Prometheus workspace. Fork and clone the sample app from the repository at aws-otel-community . This Prometheus sample app generates all 4 Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint A health check endpoint also exists at / The following is a list of optional command line flags for configuration: listen_address: (default = 0.0.0.0:8080) defines the address and port that the sample app is exposed to. This is primarily to conform with the test framework requirements. metric_count: (default=1) the amount of each type of metric to generate. The same amount of metrics is always generated per metric type. label_count: (default=1) the amount of labels per metric to generate. datapoint_count: (default=1) the number of data-points per metric to generate. Enabling Metric collection using AWS Distro for Opentelemetry \u00b6 Fork and clone the sample app from the repository at aws-otel-community. Then run the following commands. cd ./sample-apps/prometheus docker build . -t prometheus-sample-app:latest 2. Push this image to a registry such as Amazon ECR. You can use the following command to create a new ECR repository in your account. Make sure to set as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION> 3. Deploy the sample app in the cluster by copying this Kubernetes configuration and applying it. Change the image to the image that you just pushed by replacing PUBLIC_SAMPLE_APP_IMAGE in the prometheus-sample-app.yaml file. curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-sample-app.yaml -o prometheus-sample-app.yaml kubectl apply -f prometheus-sample-app.yaml 4. Start a default instance of the ADOT Collector. To do so, first enter the following command to pull the Kubernetes configuration for ADOT Collector. curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-daemonset.yaml -o prometheus-daemonset.yaml Then edit the template file, substituting the remote_write endpoint for your Amazon Managed Service for Prometheus workspace for YOUR_ENDPOINT and your Region for YOUR_REGION . Use the remote_write endpoint that is displayed in the Amazon Managed Service for Prometheus console when you look at your workspace details. You'll also need to change YOUR_ACCOUNT_ID in the service account section of the Kubernetes configuration to your AWS account ID. In this recipe, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. 5. Enter the following command to deploy the ADOT collector. kubectl apply -f eks-prometheus-daemonset.yaml Configure workspace with Terraform \u00b6 Now, we will provision a Amazon Managed Service for Prometheus workspace and will define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in expr ) holds true for a specified time period ( for ). Code in the Terraform language is stored in plain text files with the .tf file extension. There is also a JSON-based variant of the language that is named with the .tf.json file extension. We will now use the main.tf to deploy the resources using terraform. Before running the terraform command, we will export the region and sns_topic variable. export TF_VAR_region=<your region> export TF_VAR_sns_topic=<ARN of the SNS topic used by the SNS receiver> Now, we will execute the below commands to provision the workspace: terraform init terraform plan terraform apply Once the above steps are complete, verify the setup end-to-end by using awscurl and query the endpoint. Ensure the WORKSPACE_ID variable is replaced with the appropriate Amazon Managed Service for Prometheus workspace id. On running the below command, look for the metric \u201cmetric:recording_rule\u201d, and, if you successfully find the metric, then you\u2019ve successfully created a recording rule: awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/api/v1/rules --service=\"aps\" Sample Output: \"status\":\"success\",\"data\":{\"groups\":[{\"name\":\"alert-test\",\"file\":\"rules\",\"rules\":[{\"state\":\"firing\",\"name\":\"metric:alerting_rule\",\"query\":\"rate(adot_test_counter0[5m]) \\u003e 5\",\"duration\":0,\"labels\":{},\"annotations\":{},\"alerts\":[{\"labels\":{\"alertname\":\"metric:alerting_rule\"},\"annotations\":{},\"state\":\"firing\",\"activeAt\":\"2021-09-16T13:20:35.9664022Z\",\"value\":\"6.96890019778219e+01\"}],\"health\":\"ok\",\"lastError\":\"\",\"type\":\"alerting\",\"lastEvaluation\":\"2021-09-16T18:41:35.967122005Z\",\"evaluationTime\":0.018121408}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:41:35.967104769Z\",\"evaluationTime\":0.018142997},{\"name\":\"test\",\"file\":\"rules\",\"rules\":[{\"name\":\"metric:recording_rule\",\"query\":\"rate(adot_test_counter0[5m])\",\"labels\":{},\"health\":\"ok\",\"lastError\":\"\",\"type\":\"recording\",\"lastEvaluation\":\"2021-09-16T18:40:44.650001548Z\",\"evaluationTime\":0.018381387}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:40:44.649986468Z\",\"evaluationTime\":0.018400463}]},\"errorType\":\"\",\"error\":\"\"} We can further query the alertmanager endpoint to confirm the same awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/alertmanager/api/v2/alerts --service=\"aps\" -H \"Content-Type: application/json\" Sample Output: [{\"annotations\":{},\"endsAt\":\"2021-09-16T18:48:35.966Z\",\"fingerprint\":\"114212a24ca97549\",\"receivers\":[{\"name\":\"default\"}],\"startsAt\":\"2021-09-16T13:20:35.966Z\",\"status\":{\"inhibitedBy\":[],\"silencedBy\":[],\"state\":\"active\"},\"updatedAt\":\"2021-09-16T18:44:35.984Z\",\"generatorURL\":\"/graph?g0.expr=sum%28rate%28envoy_http_downstream_rq_time_bucket%5B1m%5D%29%29+%3E+5\\u0026g0.tab=1\",\"labels\":{\"alertname\":\"metric:alerting_rule\"}}] This confirms the alert was triggered and sent to SNS via the SNS receiver Clean up \u00b6 Run the following command to terminate the Amazon Managed Service for Prometheus workspace. Make sure you delete the EKS Cluster that was created as well: terraform destroy","title":"Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager"},{"location":"recipes/amp-alertmanager-terraform/#terraform-as-infrastructure-as-a-code-to-deploy-amazon-managed-service-for-prometheus-and-configure-alert-manager","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe, we will demonstrate how you can use Terraform to provision Amazon Managed Service for Prometheus and configure rules management and alert manager to send notification to a SNS topic if a certain condition is met. Note This guide will take approximately 30 minutes to complete.","title":"Terraform as Infrastructure as a Code to deploy Amazon Managed Service for Prometheus and configure Alert manager"},{"location":"recipes/amp-alertmanager-terraform/#prerequisites","text":"You will need the following to complete the setup: Amazon EKS cluster AWS CLI version 2 Terraform CLI AWS Distro for OpenTelemetry(ADOT) eksctl kubectl jq helm SNS topic awscurl In the recipe, we will use a sample application in order to demonstrate the metric scraping using ADOT and remote write the metrics to the Amazon Managed Service for Prometheus workspace. Fork and clone the sample app from the repository at aws-otel-community . This Prometheus sample app generates all 4 Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint A health check endpoint also exists at / The following is a list of optional command line flags for configuration: listen_address: (default = 0.0.0.0:8080) defines the address and port that the sample app is exposed to. This is primarily to conform with the test framework requirements. metric_count: (default=1) the amount of each type of metric to generate. The same amount of metrics is always generated per metric type. label_count: (default=1) the amount of labels per metric to generate. datapoint_count: (default=1) the number of data-points per metric to generate.","title":"Prerequisites"},{"location":"recipes/amp-alertmanager-terraform/#enabling-metric-collection-using-aws-distro-for-opentelemetry","text":"Fork and clone the sample app from the repository at aws-otel-community. Then run the following commands. cd ./sample-apps/prometheus docker build . -t prometheus-sample-app:latest 2. Push this image to a registry such as Amazon ECR. You can use the following command to create a new ECR repository in your account. Make sure to set as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION> 3. Deploy the sample app in the cluster by copying this Kubernetes configuration and applying it. Change the image to the image that you just pushed by replacing PUBLIC_SAMPLE_APP_IMAGE in the prometheus-sample-app.yaml file. curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-sample-app.yaml -o prometheus-sample-app.yaml kubectl apply -f prometheus-sample-app.yaml 4. Start a default instance of the ADOT Collector. To do so, first enter the following command to pull the Kubernetes configuration for ADOT Collector. curl https://raw.githubusercontent.com/aws-observability/aws-otel-collector/main/examples/eks/aws-prometheus/prometheus-daemonset.yaml -o prometheus-daemonset.yaml Then edit the template file, substituting the remote_write endpoint for your Amazon Managed Service for Prometheus workspace for YOUR_ENDPOINT and your Region for YOUR_REGION . Use the remote_write endpoint that is displayed in the Amazon Managed Service for Prometheus console when you look at your workspace details. You'll also need to change YOUR_ACCOUNT_ID in the service account section of the Kubernetes configuration to your AWS account ID. In this recipe, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. 5. Enter the following command to deploy the ADOT collector. kubectl apply -f eks-prometheus-daemonset.yaml","title":"Enabling Metric collection using AWS Distro for Opentelemetry"},{"location":"recipes/amp-alertmanager-terraform/#configure-workspace-with-terraform","text":"Now, we will provision a Amazon Managed Service for Prometheus workspace and will define an alerting rule that causes the Alert Manager to send a notification if a certain condition (defined in expr ) holds true for a specified time period ( for ). Code in the Terraform language is stored in plain text files with the .tf file extension. There is also a JSON-based variant of the language that is named with the .tf.json file extension. We will now use the main.tf to deploy the resources using terraform. Before running the terraform command, we will export the region and sns_topic variable. export TF_VAR_region=<your region> export TF_VAR_sns_topic=<ARN of the SNS topic used by the SNS receiver> Now, we will execute the below commands to provision the workspace: terraform init terraform plan terraform apply Once the above steps are complete, verify the setup end-to-end by using awscurl and query the endpoint. Ensure the WORKSPACE_ID variable is replaced with the appropriate Amazon Managed Service for Prometheus workspace id. On running the below command, look for the metric \u201cmetric:recording_rule\u201d, and, if you successfully find the metric, then you\u2019ve successfully created a recording rule: awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/api/v1/rules --service=\"aps\" Sample Output: \"status\":\"success\",\"data\":{\"groups\":[{\"name\":\"alert-test\",\"file\":\"rules\",\"rules\":[{\"state\":\"firing\",\"name\":\"metric:alerting_rule\",\"query\":\"rate(adot_test_counter0[5m]) \\u003e 5\",\"duration\":0,\"labels\":{},\"annotations\":{},\"alerts\":[{\"labels\":{\"alertname\":\"metric:alerting_rule\"},\"annotations\":{},\"state\":\"firing\",\"activeAt\":\"2021-09-16T13:20:35.9664022Z\",\"value\":\"6.96890019778219e+01\"}],\"health\":\"ok\",\"lastError\":\"\",\"type\":\"alerting\",\"lastEvaluation\":\"2021-09-16T18:41:35.967122005Z\",\"evaluationTime\":0.018121408}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:41:35.967104769Z\",\"evaluationTime\":0.018142997},{\"name\":\"test\",\"file\":\"rules\",\"rules\":[{\"name\":\"metric:recording_rule\",\"query\":\"rate(adot_test_counter0[5m])\",\"labels\":{},\"health\":\"ok\",\"lastError\":\"\",\"type\":\"recording\",\"lastEvaluation\":\"2021-09-16T18:40:44.650001548Z\",\"evaluationTime\":0.018381387}],\"interval\":60,\"lastEvaluation\":\"2021-09-16T18:40:44.649986468Z\",\"evaluationTime\":0.018400463}]},\"errorType\":\"\",\"error\":\"\"} We can further query the alertmanager endpoint to confirm the same awscurl https://aps-workspaces.us-east-1.amazonaws.com/workspaces/$WORKSPACE_ID/alertmanager/api/v2/alerts --service=\"aps\" -H \"Content-Type: application/json\" Sample Output: [{\"annotations\":{},\"endsAt\":\"2021-09-16T18:48:35.966Z\",\"fingerprint\":\"114212a24ca97549\",\"receivers\":[{\"name\":\"default\"}],\"startsAt\":\"2021-09-16T13:20:35.966Z\",\"status\":{\"inhibitedBy\":[],\"silencedBy\":[],\"state\":\"active\"},\"updatedAt\":\"2021-09-16T18:44:35.984Z\",\"generatorURL\":\"/graph?g0.expr=sum%28rate%28envoy_http_downstream_rq_time_bucket%5B1m%5D%29%29+%3E+5\\u0026g0.tab=1\",\"labels\":{\"alertname\":\"metric:alerting_rule\"}}] This confirms the alert was triggered and sent to SNS via the SNS receiver","title":"Configure workspace with Terraform"},{"location":"recipes/amp-alertmanager-terraform/#clean-up","text":"Run the following command to terminate the Amazon Managed Service for Prometheus workspace. Make sure you delete the EKS Cluster that was created as well: terraform destroy","title":"Clean up"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/","text":"Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Customers want to migrate their existing Prometheus workloads to the cloud and utilize all that the cloud offers. AWS has services like Amazon EC2 Auto Scaling , which lets you scale out Amazon Elastic Compute Cloud (Amazon EC2) instances based on metrics like CPU or memory utilization. Applications that use Prometheus metrics can easily integrate into EC2 Auto Scaling without needing to replace their monitoring stack. In this post, I will walk you through configuring Amazon EC2 Auto Scaling to work with Amazon Managed Service for Prometheus Alert Manager . This approach lets you move a Prometheus-based workload to the cloud while taking advantage of services like autoscaling. Amazon Managed Service for Prometheus provides support for alerting rules that use PromQL . The Prometheus alerting rules documentation provides the syntax and examples of valid alerting rules. Likewise, the Prometheus alert manager documentation references both the syntax and examples of valid alert manager configurations. Solution overview \u00b6 First, let\u2019s briefly review Amazon EC2 Auto Scaling\u2018s concept of an Auto Scaling group which is a logical collection of Amazon EC2 instances. An Auto Scaling group can launch EC2 instances based on a predefined launch template. The launch template contains information used to launch the Amazon EC2 instance, including the AMI ID, the instance type, network settings, and AWS Identity and Access Management (IAM) instance profile. Amazon EC2 Auto Scaling groups have a minimum size, maximum size, and desired capacity concepts. When Amazon EC2 Auto Scaling detects that the current running capacity of the Auto Scaling group is above or below the desired capacity, it will automatically scale out or scale in as needed. This scaling approach lets you utilize elasticity within your workload while still keeping bounds on both capacity and costs. To demonstrate this solution, I have created an Amazon EC2 Auto Scaling group that contains two Amazon EC2 instances. These instances remote write instance metrics to an Amazon Managed Service for Prometheus workspace. I have set the Auto Scaling group\u2019s minimum size to two (to maintain high availability), and I\u2019ve set the group\u2019s maximum size to 10 (to help control costs). As more traffic hits the solution, additional Amazon EC2 instances are automatically added to support the load, up to the Amazon EC2 Auto Scaling group\u2019s maximum size. As the load decreases, those Amazon EC2 instances are terminated until the Amazon EC2 Auto Scaling group reaches the group\u2019s minimum size. This approach lets you have a performant application by utilizing the elasticity of the cloud. Note that as you scrape more and more resources, you could quickly overwhelm the capabilities of a single Prometheus server. You can avoid this situation by scaling Prometheus servers linearly with the workload. This approach ensures that you can collect metric data at the granularity that you want. To support the Auto Scaling of a Prometheus workload, I have created an Amazon Managed Service for Prometheus workspace with the following rules: YAML groups: - name: example rules: - alert: HostHighCpuLoad expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) > 60 for: 5m labels: severity: warning event_type: scale_up annotations: summary: Host high CPU load (instance {{ $labels.instance }}) description: \"CPU load is > 60%\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" - alert: HostLowCpuLoad expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) < 30 for: 5m labels: severity: warning event_type: scale_down annotations: summary: Host low CPU load (instance {{ $labels.instance }}) description: \"CPU load is < 30%\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" This rules set creates a HostHighCpuLoad and a HostLowCpuLoad rules. These alerts trigger when the CPU is greater than 60% or less than 30% utilization over a five-minute period. After raising an alert, the alert manager will forward the message into an Amazon SNS topic, passing an alert_type (the alert name) and event_type (scale_down or scale_up). YAML alertmanager_config: | route: receiver: default_receiver repeat_interval: 5m receivers: - name: default_receiver sns_configs: - topic_arn: <ARN OF SNS TOPIC GOES HERE> send_resolved: false sigv4: region: us-east-1 message: | alert_type: {{ .CommonLabels.alertname }} event_type: {{ .CommonLabels.event_type }} An AWS Lambda function is subscribed to the Amazon SNS topic. I have written logic in the Lambda function to inspect the Amazon SNS message and determine if a scale_up or scale_down event should happen. Then, the Lambda function increments or decrements the desired capacity of the Amazon EC2 Auto Scaling group. The Amazon EC2 Auto Scaling group detects a requested change in capacity, and then invokes or deallocates Amazon EC2 instances. The Lambda code to support Auto Scaling is as follows: Python import json import boto3 import os def lambda_handler(event, context): print(event) msg = event['Records'][0]['Sns']['Message'] scale_type = '' if msg.find('scale_up') > -1: scale_type = 'scale_up' else: scale_type = 'scale_down' get_desired_instance_count(scale_type) def get_desired_instance_count(scale_type): client = boto3.client('autoscaling') asg_name = os.environ['ASG_NAME'] response = client.describe_auto_scaling_groups(AutoScalingGroupNames=[ asg_name]) minSize = response['AutoScalingGroups'][0]['MinSize'] maxSize = response['AutoScalingGroups'][0]['MaxSize'] desiredCapacity = response['AutoScalingGroups'][0]['DesiredCapacity'] if scale_type == \"scale_up\": desiredCapacity = min(desiredCapacity+1, maxSize) if scale_type == \"scale_down\": desiredCapacity = max(desiredCapacity - 1, minSize) print('Scale type: {}; new capacity: {}'.format(scale_type, desiredCapacity)) response = client.set_desired_capacity(AutoScalingGroupName=asg_name, DesiredCapacity=desiredCapacity, HonorCooldown=False) The full architecture can be reviewed in the following figure. Testing out the solution \u00b6 You can launch an AWS CloudFormation template to automatically provision this solution. Stack prerequisites: An Amazon Virtual Private Cloud (Amazon VPC) An AWS Security Group that allows outbound traffic Select the Download Launch Stack Template link to download and set up the template in your account. As part of the configuration process, you must specify the subnets and the security groups that you want associated with the Amazon EC2 instances. See the following figure for details. ## Download Launch Stack Template This is the CloudFormation stack details screen, where the stack name has been set as prometheus-autoscale. The stack parameters include a URL of the Linux installer for Prometheus, the URL for the Linux Node Exporter for Prometheus, the subnets and security groups used in the solution, the AMI and instance type to use, and the maximum capacity of the Amazon EC2 Auto Scaling group. The stack will take approximately eight minutes to deploy. Once complete, you will find two Amazon EC2 instances that have been deployed and are running in the Amazon EC2 Auto Scaling group that has been created for you. To validate that this solution auto-scales via Amazon Managed Service for Prometheus Alert Manager, you apply load to the Amazon EC2 instances using the AWS Systems Manager Run Command and the AWSFIS-Run-CPU-Stress automation document . As stress is applied to the CPUs in the Amazon EC2 Auto Scaling group, alert manager publishes these alerts, which the Lambda function responds to by scaling up the Auto Scaling group. As CPU consumption decreases, the low CPU alert in the Amazon Managed Service for Prometheus workspace fires, alert manager publishes the alert to the Amazon SNS topic, and the Lambda function responds by responds by scaling down the Auto Scaling group, as demonstrated in the following figure. The Grafana dashboard has a line showing that CPU has spiked to 100%. Although the CPU is high, another line shows that the number of instances has stepped up from 2 to 10. Once CPU has decreased, the number of instances slowly decreases back down to 2. Costs \u00b6 Amazon Managed Service for Prometheus is priced based on the metrics ingested, metrics stored, and metrics queried. Visit the Amazon Managed Service for Prometheus pricing page for the latest pricing and pricing examples. Amazon SNS is priced based on the number of monthly API requests made. Message delivery between Amazon SNS and Lambda is free, but it does charge for the amount of data transferred between Amazon SNS and Lambda. See the latest Amazon SNS pricing details . Lambda is priced based on the duration of your function execution and the number of requests made to the function. See the latest AWS Lambda pricing details . There are no additional charges for using Amazon EC2 Auto Scaling. Conclusion \u00b6 By using Amazon Managed Service for Prometheus, alert manager, Amazon SNS, and Lambda, you can control the scaling activities of an Amazon EC2 Auto Scaling group. The solution in this post demonstrates how you can move existing Prometheus workloads to AWS, while also utilizing Amazon EC2 Auto Scaling. As load increases to the application, it seamlessly scales to meet demand. In this example, the Amazon EC2 Auto Scaling group scaled based on CPU, but you can follow a similar approach for any Prometheus metric from your workload. This approach provides fine-grained control over scaling actions, thereby making sure that you can scale your workload on the metric that provides the most business value. In previous blog posts, we\u2019ve also demonstrated how you can use Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty and how to integrate Amazon Managed Service for Prometheus with Slack . These solutions show how you can receive alerts from your workspace in the way that is most useful to you. For next steps, see how to create your own rules configuration file for Amazon Managed Service for Prometheus, and set up your own alert receiver . Moreover, check out Awesome Prometheus alerts for some good examples of alerting rules that can be used within alert manager.","title":"Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/#auto-scaling-amazon-ec2-using-amazon-managed-service-for-prometheus-and-alert-manager","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Customers want to migrate their existing Prometheus workloads to the cloud and utilize all that the cloud offers. AWS has services like Amazon EC2 Auto Scaling , which lets you scale out Amazon Elastic Compute Cloud (Amazon EC2) instances based on metrics like CPU or memory utilization. Applications that use Prometheus metrics can easily integrate into EC2 Auto Scaling without needing to replace their monitoring stack. In this post, I will walk you through configuring Amazon EC2 Auto Scaling to work with Amazon Managed Service for Prometheus Alert Manager . This approach lets you move a Prometheus-based workload to the cloud while taking advantage of services like autoscaling. Amazon Managed Service for Prometheus provides support for alerting rules that use PromQL . The Prometheus alerting rules documentation provides the syntax and examples of valid alerting rules. Likewise, the Prometheus alert manager documentation references both the syntax and examples of valid alert manager configurations.","title":"Auto-scaling Amazon EC2 using Amazon Managed Service for Prometheus and alert manager"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/#solution-overview","text":"First, let\u2019s briefly review Amazon EC2 Auto Scaling\u2018s concept of an Auto Scaling group which is a logical collection of Amazon EC2 instances. An Auto Scaling group can launch EC2 instances based on a predefined launch template. The launch template contains information used to launch the Amazon EC2 instance, including the AMI ID, the instance type, network settings, and AWS Identity and Access Management (IAM) instance profile. Amazon EC2 Auto Scaling groups have a minimum size, maximum size, and desired capacity concepts. When Amazon EC2 Auto Scaling detects that the current running capacity of the Auto Scaling group is above or below the desired capacity, it will automatically scale out or scale in as needed. This scaling approach lets you utilize elasticity within your workload while still keeping bounds on both capacity and costs. To demonstrate this solution, I have created an Amazon EC2 Auto Scaling group that contains two Amazon EC2 instances. These instances remote write instance metrics to an Amazon Managed Service for Prometheus workspace. I have set the Auto Scaling group\u2019s minimum size to two (to maintain high availability), and I\u2019ve set the group\u2019s maximum size to 10 (to help control costs). As more traffic hits the solution, additional Amazon EC2 instances are automatically added to support the load, up to the Amazon EC2 Auto Scaling group\u2019s maximum size. As the load decreases, those Amazon EC2 instances are terminated until the Amazon EC2 Auto Scaling group reaches the group\u2019s minimum size. This approach lets you have a performant application by utilizing the elasticity of the cloud. Note that as you scrape more and more resources, you could quickly overwhelm the capabilities of a single Prometheus server. You can avoid this situation by scaling Prometheus servers linearly with the workload. This approach ensures that you can collect metric data at the granularity that you want. To support the Auto Scaling of a Prometheus workload, I have created an Amazon Managed Service for Prometheus workspace with the following rules: YAML groups: - name: example rules: - alert: HostHighCpuLoad expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) > 60 for: 5m labels: severity: warning event_type: scale_up annotations: summary: Host high CPU load (instance {{ $labels.instance }}) description: \"CPU load is > 60%\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" - alert: HostLowCpuLoad expr: 100 - (avg(rate(node_cpu_seconds_total{mode=\"idle\"}[2m])) * 100) < 30 for: 5m labels: severity: warning event_type: scale_down annotations: summary: Host low CPU load (instance {{ $labels.instance }}) description: \"CPU load is < 30%\\n VALUE = {{ $value }}\\n LABELS = {{ $labels }}\" This rules set creates a HostHighCpuLoad and a HostLowCpuLoad rules. These alerts trigger when the CPU is greater than 60% or less than 30% utilization over a five-minute period. After raising an alert, the alert manager will forward the message into an Amazon SNS topic, passing an alert_type (the alert name) and event_type (scale_down or scale_up). YAML alertmanager_config: | route: receiver: default_receiver repeat_interval: 5m receivers: - name: default_receiver sns_configs: - topic_arn: <ARN OF SNS TOPIC GOES HERE> send_resolved: false sigv4: region: us-east-1 message: | alert_type: {{ .CommonLabels.alertname }} event_type: {{ .CommonLabels.event_type }} An AWS Lambda function is subscribed to the Amazon SNS topic. I have written logic in the Lambda function to inspect the Amazon SNS message and determine if a scale_up or scale_down event should happen. Then, the Lambda function increments or decrements the desired capacity of the Amazon EC2 Auto Scaling group. The Amazon EC2 Auto Scaling group detects a requested change in capacity, and then invokes or deallocates Amazon EC2 instances. The Lambda code to support Auto Scaling is as follows: Python import json import boto3 import os def lambda_handler(event, context): print(event) msg = event['Records'][0]['Sns']['Message'] scale_type = '' if msg.find('scale_up') > -1: scale_type = 'scale_up' else: scale_type = 'scale_down' get_desired_instance_count(scale_type) def get_desired_instance_count(scale_type): client = boto3.client('autoscaling') asg_name = os.environ['ASG_NAME'] response = client.describe_auto_scaling_groups(AutoScalingGroupNames=[ asg_name]) minSize = response['AutoScalingGroups'][0]['MinSize'] maxSize = response['AutoScalingGroups'][0]['MaxSize'] desiredCapacity = response['AutoScalingGroups'][0]['DesiredCapacity'] if scale_type == \"scale_up\": desiredCapacity = min(desiredCapacity+1, maxSize) if scale_type == \"scale_down\": desiredCapacity = max(desiredCapacity - 1, minSize) print('Scale type: {}; new capacity: {}'.format(scale_type, desiredCapacity)) response = client.set_desired_capacity(AutoScalingGroupName=asg_name, DesiredCapacity=desiredCapacity, HonorCooldown=False) The full architecture can be reviewed in the following figure.","title":"Solution overview"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/#testing-out-the-solution","text":"You can launch an AWS CloudFormation template to automatically provision this solution. Stack prerequisites: An Amazon Virtual Private Cloud (Amazon VPC) An AWS Security Group that allows outbound traffic Select the Download Launch Stack Template link to download and set up the template in your account. As part of the configuration process, you must specify the subnets and the security groups that you want associated with the Amazon EC2 instances. See the following figure for details. ## Download Launch Stack Template This is the CloudFormation stack details screen, where the stack name has been set as prometheus-autoscale. The stack parameters include a URL of the Linux installer for Prometheus, the URL for the Linux Node Exporter for Prometheus, the subnets and security groups used in the solution, the AMI and instance type to use, and the maximum capacity of the Amazon EC2 Auto Scaling group. The stack will take approximately eight minutes to deploy. Once complete, you will find two Amazon EC2 instances that have been deployed and are running in the Amazon EC2 Auto Scaling group that has been created for you. To validate that this solution auto-scales via Amazon Managed Service for Prometheus Alert Manager, you apply load to the Amazon EC2 instances using the AWS Systems Manager Run Command and the AWSFIS-Run-CPU-Stress automation document . As stress is applied to the CPUs in the Amazon EC2 Auto Scaling group, alert manager publishes these alerts, which the Lambda function responds to by scaling up the Auto Scaling group. As CPU consumption decreases, the low CPU alert in the Amazon Managed Service for Prometheus workspace fires, alert manager publishes the alert to the Amazon SNS topic, and the Lambda function responds by responds by scaling down the Auto Scaling group, as demonstrated in the following figure. The Grafana dashboard has a line showing that CPU has spiked to 100%. Although the CPU is high, another line shows that the number of instances has stepped up from 2 to 10. Once CPU has decreased, the number of instances slowly decreases back down to 2.","title":"Testing out the solution"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/#costs","text":"Amazon Managed Service for Prometheus is priced based on the metrics ingested, metrics stored, and metrics queried. Visit the Amazon Managed Service for Prometheus pricing page for the latest pricing and pricing examples. Amazon SNS is priced based on the number of monthly API requests made. Message delivery between Amazon SNS and Lambda is free, but it does charge for the amount of data transferred between Amazon SNS and Lambda. See the latest Amazon SNS pricing details . Lambda is priced based on the duration of your function execution and the number of requests made to the function. See the latest AWS Lambda pricing details . There are no additional charges for using Amazon EC2 Auto Scaling.","title":"Costs"},{"location":"recipes/as-ec2-using-amp-and-alertmanager/#conclusion","text":"By using Amazon Managed Service for Prometheus, alert manager, Amazon SNS, and Lambda, you can control the scaling activities of an Amazon EC2 Auto Scaling group. The solution in this post demonstrates how you can move existing Prometheus workloads to AWS, while also utilizing Amazon EC2 Auto Scaling. As load increases to the application, it seamlessly scales to meet demand. In this example, the Amazon EC2 Auto Scaling group scaled based on CPU, but you can follow a similar approach for any Prometheus metric from your workload. This approach provides fine-grained control over scaling actions, thereby making sure that you can scale your workload on the metric that provides the most business value. In previous blog posts, we\u2019ve also demonstrated how you can use Amazon Managed Service for Prometheus Alert Manager to receive alerts with PagerDuty and how to integrate Amazon Managed Service for Prometheus with Slack . These solutions show how you can receive alerts from your workspace in the way that is most useful to you. For next steps, see how to create your own rules configuration file for Amazon Managed Service for Prometheus, and set up your own alert receiver . Moreover, check out Awesome Prometheus alerts for some good examples of alerting rules that can be used within alert manager.","title":"Conclusion"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/","text":"Using AWS Distro for OpenTelemetry in EKS on EC2 with Amazon Managed Service for Prometheus \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) on EC2 cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus. The ADOT Collector includes two components specific to Prometheus: the Prometheus Receiver, and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment. Create EKS on EC2 cluster \u00b6 Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster-config.yaml . This template will create a new cluster with two EC2 t2.large nodes. Edit the template file and set <YOUR_REGION> to one of the supported regions for AMP . Make sure to overwrite <YOUR_REGION> in your session, for example in bash: export AWS_DEFAULT_REGION=<YOUR_REGION> Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml Set up an ECR repository \u00b6 In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. Make sure to set <YOUR_REGION> as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION> Set up AMP \u00b6 create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide. Set up ADOT Collector \u00b6 Download adot-collector-ec2.yaml and edit this YAML doc with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <YOUR_REGION> with your current region. 2. Replace <YOUR_ENDPOINT> with the remote write URL of your workspace. Get your AMP remote write URL endpoint by executing the following queries. First, get the workspace ID like so: YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\ --alias prometheus-sample-app \\ --query 'workspaces[0].workspaceId' --output text) Now get the remote write URL endpoint URL for your workspace using: YOUR_ENDPOINT=$(aws amp describe-workspace \\ --workspace-id $YOUR_WORKSPACE_ID \\ --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write Warning Make sure that YOUR_ENDPOINT is in fact the remote write URL, that is, the URL should end in /api/v1/remote_write . After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f adot-collector-ec2.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup . Set up AMG \u00b6 Setup a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation. Application \u00b6 In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint. Build container image \u00b6 To build the container image, first clone the Git repository and change into the directory as follows: git clone https://github.com/aws-observability/aws-otel-community.git && \\ cd ./aws-otel-community/sample-apps/prometheus First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <YOUR_REGION> with your current region. For example, in the Bash shell this would look as follows: export AWS_DEFAULT_REGION=<YOUR_REGION> export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Next, build the container image: docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Now you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Deploy sample app \u00b6 Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and AWS_DEFAULT_REGION in the file with your own values: # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Now you can deploy the sample app to your cluster using: kubectl apply -f prometheus-sample-app.yaml End-to-end \u00b6 Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG. Verify your pipeline is working \u00b6 To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs. Enter the following command to follow the ADOT collector logs: kubectl -n adot-col logs adot-collector -f One example output in the logs of the scraped metrics from the sample app should look like the following: ... Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 ... Tip To verify if AMP received the metrics, you can use awscurl . This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace $AMP_ENDPOINT with the endpoint for your AMP workspace: $ awscurl --service=\"aps\" \\ --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}} Create a Grafana dashboard \u00b6 You can import an example dashboard, available via prometheus-sample-app-dashboard.json , for the sample app that looks as follows: Further, use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on EC2 to ingest metrics. Cleanup \u00b6 Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-ec2 Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Using AWS Distro for OpenTelemetry in EKS on EC2 with Amazon Managed Service for Prometheus"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#using-aws-distro-for-opentelemetry-in-eks-on-ec2-with-amazon-managed-service-for-prometheus","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus (AMP) . Then we're using Amazon Managed Grafana (AMG) to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) on EC2 cluster and Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete.","title":"Using AWS Distro for OpenTelemetry in EKS on EC2 with Amazon Managed Service for Prometheus"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#architecture","text":"The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus. The ADOT Collector includes two components specific to Prometheus: the Prometheus Receiver, and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP","title":"Architecture"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have docker installed into your environment.","title":"Prerequisites"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#create-eks-on-ec2-cluster","text":"Our demo application in this recipe will be running on top of EKS. You can either use an existing EKS cluster or create one using cluster-config.yaml . This template will create a new cluster with two EC2 t2.large nodes. Edit the template file and set <YOUR_REGION> to one of the supported regions for AMP . Make sure to overwrite <YOUR_REGION> in your session, for example in bash: export AWS_DEFAULT_REGION=<YOUR_REGION> Create your cluster using the following command. eksctl create cluster -f cluster-config.yaml","title":"Create EKS on EC2 cluster"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-an-ecr-repository","text":"In order to deploy our application to EKS we need a container registry. You can use the following command to create a new ECR registry in your account. Make sure to set <YOUR_REGION> as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION>","title":"Set up an ECR repository"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amp","text":"create a workspace using the AWS CLI aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide.","title":"Set up AMP"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-adot-collector","text":"Download adot-collector-ec2.yaml and edit this YAML doc with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <YOUR_REGION> with your current region. 2. Replace <YOUR_ENDPOINT> with the remote write URL of your workspace. Get your AMP remote write URL endpoint by executing the following queries. First, get the workspace ID like so: YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\ --alias prometheus-sample-app \\ --query 'workspaces[0].workspaceId' --output text) Now get the remote write URL endpoint URL for your workspace using: YOUR_ENDPOINT=$(aws amp describe-workspace \\ --workspace-id $YOUR_WORKSPACE_ID \\ --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write Warning Make sure that YOUR_ENDPOINT is in fact the remote write URL, that is, the URL should end in /api/v1/remote_write . After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f adot-collector-ec2.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup .","title":"Set up ADOT Collector"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#set-up-amg","text":"Setup a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.","title":"Set up AMG"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#application","text":"In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint.","title":"Application"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#build-container-image","text":"To build the container image, first clone the Git repository and change into the directory as follows: git clone https://github.com/aws-observability/aws-otel-community.git && \\ cd ./aws-otel-community/sample-apps/prometheus First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <YOUR_REGION> with your current region. For example, in the Bash shell this would look as follows: export AWS_DEFAULT_REGION=<YOUR_REGION> export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Next, build the container image: docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Now you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"","title":"Build container image"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#deploy-sample-app","text":"Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and AWS_DEFAULT_REGION in the file with your own values: # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Now you can deploy the sample app to your cluster using: kubectl apply -f prometheus-sample-app.yaml","title":"Deploy sample app"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#end-to-end","text":"Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.","title":"End-to-end"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","text":"To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs. Enter the following command to follow the ADOT collector logs: kubectl -n adot-col logs adot-collector -f One example output in the logs of the scraped metrics from the sample app should look like the following: ... Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 ... Tip To verify if AMP received the metrics, you can use awscurl . This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace $AMP_ENDPOINT with the endpoint for your AMP workspace: $ awscurl --service=\"aps\" \\ --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}","title":"Verify your pipeline is working"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","text":"You can import an example dashboard, available via prometheus-sample-app-dashboard.json , for the sample app that looks as follows: Further, use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on EC2 to ingest metrics.","title":"Create a Grafana dashboard"},{"location":"recipes/ec2-eks-metrics-go-adot-ampamg/#cleanup","text":"Remove the resources and cluster kubectl delete all --all eksctl delete cluster --name amp-eks-ec2 Remove the AMP workspace aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/eks-observability-accelerator/","text":"Introducing Amazon EKS Observability Accelerator \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Observability is critical for any application and understanding system behavior and performance. It takes time and effort to detect and remediate performance slowdowns or disruptions. Customers often spend much time writing configuration files and work quite to achieve end-to-end monitoring for applications. Infrastructure as Code (IaC) tools, such as AWS CloudFormation , Terraform , and Ansible , reduce manual efforts by helping administrators and developers instantiate infrastructure using configuration files. Amazon Elastic Kubernetes Service (Amazon EKS) is a powerful and extensible container orchestration technology that lets you deploy and manage containerized applications at scale. Building a tailored Amazon EKS cluster amidst the wide range of tooling and design choices available and making sure that it meets your application\u2019s specific needs can take a significant amount of time. This situation becomes even more cumbersome when you implement observability, which is critical for analyzing any application\u2019s performance. Customers have been asking us for examples demonstrating the integration of various open-source tools on Amazon EKS and the configuration of observability solutions incorporating best practices for specific application requirements. On May 17, 2022, AWS announced EKS Observability Accelerator, which is used to configure and deploy purpose-built observability solutions on Amazon EKS clusters for specific workloads using Terraform modules. Customers can use this solution to get started with Amazon Managed Service for Prometheus , AWS Distro for OpenTelemetry , and Amazon Managed Grafana by running a single command and beginning to monitor applications. We built the Terraform modules to enable observability on Amazon EKS clusters for the following workloads: Java/JMX NGINX Memcached HAProxy AWS will continue to add examples for more workloads in the future. In this post, you will walk through the steps for using EKS Observability Accelerator to build the Amazon EKS cluster and configure opinionated observability components to monitor specific workloads, which is a Java/JMX application. Prerequisites \u00b6 Make sure you complete the prerequisites before proceeding with this solution Install Terraform Install Kubectl Install docker AWS Command Line Interface (AWS CLI) version 2 jq An AWS Account Configure the credentials in AWS CLI An existing Amazon Managed Grafana Workspace Deployment steps \u00b6 Imagine that you\u2019re a Kubernetes operator and in charge of provisioning the Kubernetes environment for your organization. The requirements you get from teams can be diverse and require spending a significant amount of time provisioning the Kubernetes environment and incorporating those configurations. The clock resets every time a new request comes, so re-inventing the wheel continues. To simplify this and reduce the work hours, we came up with EKS Blueprints . EKS Blueprints is a collection of Terraform modules that aim to make it easier and faster for customers to adopt Amazon EKS and start deploying typical workloads. It\u2019s open -source and can be used by anyone to configure and manage complete Amazon EKS clusters that are fully bootstrapped with the operational software needed to deploy and operate workloads. The EKS Blueprints repository contains The Amazon EKS Observability Accelerator module. You\u2019ll use it to configure observability for the Java/JMX application deployed on the Amazon EKS cluster. Step 1: Cloning the repository \u00b6 First, you\u2019ll clone the repository that contains the EKS blueprints: git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git Step 2: Generate a Grafana API Key \u00b6 Before we deploy the Terraform module, we\u2019ll create a Grafana API key and configure the Terraform variable file to use the keys to deploy the dashboard. We\u2019ll use an existing Amazon Managed Grafana workspace and must log in to the workspace URL to configure the API keys. Follow these steps to create the key Use your SAML/SSO credential to log in to the Amazon Managed Grafana workspace. Hover to the left side control panel, and select the API keys tab under the gear icon. Click Add API key, fill in the Name field and select the Role as Admin. Fill in the Time to live field. It\u2019s the API key life duration. For example, 1d to expire the key after one day. Supported units are: s,m,h,d,w,M,y Click Add Copy and keep the API key safe, we will use this Key in our next step Step 3: Configuring the environment \u00b6 Next, you\u2019ll configure the environment to deploy the Terraform module to provision the EKS cluster, AWS OTEL Operator, and Amazon Managed Service for Prometheus. Deploying the Terraform module involves the below steps: Plan : Terraform plan creates an execution plan and previews the infrastructure changes. Apply : Terraform executes the plan\u2019s action and modifies the environment. Next, configure the environment either by creating the variables file or setting up the environment variables. A \u201c.tfvars\u201d file is an alternative to using the \u201c-var\u201d flag or environment variables. The file defines the variable values used by the script. For this blog post, we will create a new file named \u201cdev.tfvars\u201d under ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java Make sure you edit the dev.tfvars file with corresponding Grafana workspace endpoint and Grafana API key. Also, if you want to customize the configuration, add the necessary variables to dev.tfvars file. cd ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java vi dev.tfvars grafana_endpoint=\u201d<Your Grafana workspace endpoint>\" grafana_api_key=\u201dYour Grafana API Key>\" Note API_KEY \u2013 is the key that you have created Grafana_Endpoint \u2013 Grafana workspace URL. Make sure to include with \u201chttps://\u201d otherwise, terraform module will fail. Step 4: Deploying the Terraform modules \u00b6 The first step is to initialize the working directory using terraform init command, which initializes a working directory containing Terraform configuration files. This command runs after writing a new Terraform configuration or cloning an existing one from version control. terraform init This command performs initialization steps to prepare the current working directory for use with Terraform. Once the initialization completes, you should receive the following notification. Additionally, we can execute the terraform validate command to evaluate the configuration files in a directory. Validate runs checks that verify whether or not a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. terraform validate The next step is to run the terraform plan command to create an execution plan, which lets you preview the Terraform infrastructure changes. By default, when terraform creates a plan it: Reads the current state of any already-existing remote objects to ensure that the Terraform state is up-to-date. It compares the current configuration to the initial state and reports any differences. Proposes a set of change actions that should, if applied, make the remote objects match the configuration. The plan command alone will not carry out the proposed changes. So you can use this command to check whether the proposed changes match what you expected before applying the changes or share your changes with your team for broader review. terraform plan -var-file=./dev.tfvars Finally, you\u2019ll run the terraform apply command to provision the resources, and it takes about 20 minutes to complete. This command deploys the following resources: terraform apply -var-file=./dev.tfvars -auto-approve Creates an Amazon EKS cluster named aws001-preprod-dev-eks Creates an Amazon Managed Service for Prometheus workspace named amp-ws-aws001-preprod-dev-eks Creates a Kubernetes namespace named opentelemetry-operator-system, adot-collector-java Deploys the AWS ADOT collector into the namespace with the configuration to collect metrics for Java/JMX workloads Builds a dashboard to visualize Java/JMX metrics in an existing Amazon Managed Grafana workspace specified in the earlier step and configures the Amazon Managed Service for Prometheus workspace as a data source After provisioning the EKS cluster, you\u2019ll add the Amazon EKS Cluster endpoint to the kubeconfig and verify if the resources provision successfully. aws eks --region $AWS_REGION update-kubeconfig --name aws001-preprod-dev-eks Verify the creation of Amazon Managed Service for Prometheus workspace and ADOT collector by running the following command: aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"amp-ws-aws001-preprod-dev-eks\").workspaceId' Listing all the pods from the cluster kubectl get pods -A We should be able to verify the connection between Amazon Managed Grafana and Amazon Managed Prometheus by heading to the configuration page and looking at the default data source. Select the Data source named amp Scroll down and select Save & test It should display a success message like below Step 5: Deploying sample Java/JMX application \u00b6 You\u2019ll deploy a sample Java/JMX application and start[nn1] scraping the JMX metrics. The sample application generates JMX metrics, such as the JVM memory pool, JVM memory usage, and thread, and exports it in the Prometheus format. You\u2019ll deploy a load generator and a bad load generator to get a wide range of metrics to visualize eventually. The EKS Observability accelerator collects the metrics for the AWS OTEL operator deployment. ADOT exporter will ingest these metrics into the Amazon Managed Service for Prometheus workspace. We\u2019ll reuse an example from the AWS OpenTelemetry collector repository : # Switch to home directory cd ~/ #Clone the git repository git clone https://github.com/aws-observability/aws-otel-test-framework.git #Setup environment variables export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` #Login to registry aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com #Create ECR Repository aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\ --image-scanning-configuration scanOnPush=true \\ --region $AWS_REGION #Build Docker image and push to ECR cd ~/aws-otel-test-framework/sample-apps/jmx docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest . docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest #Deploy the sample application export SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample curl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml > metrics-sample.yaml sed -e \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" metrics-sample.yaml -i sed -e \"s/{{region}}/$AWS_REGION/g\" metrics-sample.yaml -i sed -e \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" metrics-sample.yaml -i kubectl apply -f metrics-sample.yaml #Verify the application kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE Step 6: Visualize the JMX metrics on Amazon Managed Grafana \u00b6 To visualize the JMX metrics collected by the AWS ADOT operator, log in to the Grafana workspace. Select Dashboards and choose Manage Select the Observability folder and choose the dashboard named EKS Accelerator \u2013 Observability \u2013 Java/JMX The Terraform module added the Amazon Managed Service for Prometheus workspace as the default data source, and created a custom dashboard to visualize the metrics. You can also deploy sample applications for NGINX, HAProxy, and Memcached. Clean up \u00b6 Run the following command to tear down the resources provisioned by the Terraform module: terraform destroy -var-file=./dev.tfvars -auto-approve Conclusion \u00b6 Customers can now leverage EKS Observability Accelerator to deploy the opinionated EKS clusters and configure observability for specific workloads without spending much time manually deploying the resources and configuring the agent to scrape the metrics. Furthermore, the solution provides the extensibility to connect the Amazon Managed Prometheus workspace with Amazon Managed Grafana and configure alerts and notifications.","title":"Introducing Amazon EKS Observability Accelerator"},{"location":"recipes/eks-observability-accelerator/#introducing-amazon-eks-observability-accelerator","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. Observability is critical for any application and understanding system behavior and performance. It takes time and effort to detect and remediate performance slowdowns or disruptions. Customers often spend much time writing configuration files and work quite to achieve end-to-end monitoring for applications. Infrastructure as Code (IaC) tools, such as AWS CloudFormation , Terraform , and Ansible , reduce manual efforts by helping administrators and developers instantiate infrastructure using configuration files. Amazon Elastic Kubernetes Service (Amazon EKS) is a powerful and extensible container orchestration technology that lets you deploy and manage containerized applications at scale. Building a tailored Amazon EKS cluster amidst the wide range of tooling and design choices available and making sure that it meets your application\u2019s specific needs can take a significant amount of time. This situation becomes even more cumbersome when you implement observability, which is critical for analyzing any application\u2019s performance. Customers have been asking us for examples demonstrating the integration of various open-source tools on Amazon EKS and the configuration of observability solutions incorporating best practices for specific application requirements. On May 17, 2022, AWS announced EKS Observability Accelerator, which is used to configure and deploy purpose-built observability solutions on Amazon EKS clusters for specific workloads using Terraform modules. Customers can use this solution to get started with Amazon Managed Service for Prometheus , AWS Distro for OpenTelemetry , and Amazon Managed Grafana by running a single command and beginning to monitor applications. We built the Terraform modules to enable observability on Amazon EKS clusters for the following workloads: Java/JMX NGINX Memcached HAProxy AWS will continue to add examples for more workloads in the future. In this post, you will walk through the steps for using EKS Observability Accelerator to build the Amazon EKS cluster and configure opinionated observability components to monitor specific workloads, which is a Java/JMX application.","title":"Introducing Amazon EKS Observability Accelerator"},{"location":"recipes/eks-observability-accelerator/#prerequisites","text":"Make sure you complete the prerequisites before proceeding with this solution Install Terraform Install Kubectl Install docker AWS Command Line Interface (AWS CLI) version 2 jq An AWS Account Configure the credentials in AWS CLI An existing Amazon Managed Grafana Workspace","title":"Prerequisites"},{"location":"recipes/eks-observability-accelerator/#deployment-steps","text":"Imagine that you\u2019re a Kubernetes operator and in charge of provisioning the Kubernetes environment for your organization. The requirements you get from teams can be diverse and require spending a significant amount of time provisioning the Kubernetes environment and incorporating those configurations. The clock resets every time a new request comes, so re-inventing the wheel continues. To simplify this and reduce the work hours, we came up with EKS Blueprints . EKS Blueprints is a collection of Terraform modules that aim to make it easier and faster for customers to adopt Amazon EKS and start deploying typical workloads. It\u2019s open -source and can be used by anyone to configure and manage complete Amazon EKS clusters that are fully bootstrapped with the operational software needed to deploy and operate workloads. The EKS Blueprints repository contains The Amazon EKS Observability Accelerator module. You\u2019ll use it to configure observability for the Java/JMX application deployed on the Amazon EKS cluster.","title":"Deployment steps"},{"location":"recipes/eks-observability-accelerator/#step-1-cloning-the-repository","text":"First, you\u2019ll clone the repository that contains the EKS blueprints: git clone https://github.com/aws-ia/terraform-aws-eks-blueprints.git","title":"Step 1: Cloning the repository"},{"location":"recipes/eks-observability-accelerator/#step-2-generate-a-grafana-api-key","text":"Before we deploy the Terraform module, we\u2019ll create a Grafana API key and configure the Terraform variable file to use the keys to deploy the dashboard. We\u2019ll use an existing Amazon Managed Grafana workspace and must log in to the workspace URL to configure the API keys. Follow these steps to create the key Use your SAML/SSO credential to log in to the Amazon Managed Grafana workspace. Hover to the left side control panel, and select the API keys tab under the gear icon. Click Add API key, fill in the Name field and select the Role as Admin. Fill in the Time to live field. It\u2019s the API key life duration. For example, 1d to expire the key after one day. Supported units are: s,m,h,d,w,M,y Click Add Copy and keep the API key safe, we will use this Key in our next step","title":"Step 2: Generate a Grafana API Key"},{"location":"recipes/eks-observability-accelerator/#step-3-configuring-the-environment","text":"Next, you\u2019ll configure the environment to deploy the Terraform module to provision the EKS cluster, AWS OTEL Operator, and Amazon Managed Service for Prometheus. Deploying the Terraform module involves the below steps: Plan : Terraform plan creates an execution plan and previews the infrastructure changes. Apply : Terraform executes the plan\u2019s action and modifies the environment. Next, configure the environment either by creating the variables file or setting up the environment variables. A \u201c.tfvars\u201d file is an alternative to using the \u201c-var\u201d flag or environment variables. The file defines the variable values used by the script. For this blog post, we will create a new file named \u201cdev.tfvars\u201d under ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java Make sure you edit the dev.tfvars file with corresponding Grafana workspace endpoint and Grafana API key. Also, if you want to customize the configuration, add the necessary variables to dev.tfvars file. cd ~/terraform-aws-eks-blueprints/examples/observability/adot-amp-grafana-for-java vi dev.tfvars grafana_endpoint=\u201d<Your Grafana workspace endpoint>\" grafana_api_key=\u201dYour Grafana API Key>\" Note API_KEY \u2013 is the key that you have created Grafana_Endpoint \u2013 Grafana workspace URL. Make sure to include with \u201chttps://\u201d otherwise, terraform module will fail.","title":"Step 3: Configuring the environment"},{"location":"recipes/eks-observability-accelerator/#step-4-deploying-the-terraform-modules","text":"The first step is to initialize the working directory using terraform init command, which initializes a working directory containing Terraform configuration files. This command runs after writing a new Terraform configuration or cloning an existing one from version control. terraform init This command performs initialization steps to prepare the current working directory for use with Terraform. Once the initialization completes, you should receive the following notification. Additionally, we can execute the terraform validate command to evaluate the configuration files in a directory. Validate runs checks that verify whether or not a configuration is syntactically valid and internally consistent, regardless of any provided variables or existing state. terraform validate The next step is to run the terraform plan command to create an execution plan, which lets you preview the Terraform infrastructure changes. By default, when terraform creates a plan it: Reads the current state of any already-existing remote objects to ensure that the Terraform state is up-to-date. It compares the current configuration to the initial state and reports any differences. Proposes a set of change actions that should, if applied, make the remote objects match the configuration. The plan command alone will not carry out the proposed changes. So you can use this command to check whether the proposed changes match what you expected before applying the changes or share your changes with your team for broader review. terraform plan -var-file=./dev.tfvars Finally, you\u2019ll run the terraform apply command to provision the resources, and it takes about 20 minutes to complete. This command deploys the following resources: terraform apply -var-file=./dev.tfvars -auto-approve Creates an Amazon EKS cluster named aws001-preprod-dev-eks Creates an Amazon Managed Service for Prometheus workspace named amp-ws-aws001-preprod-dev-eks Creates a Kubernetes namespace named opentelemetry-operator-system, adot-collector-java Deploys the AWS ADOT collector into the namespace with the configuration to collect metrics for Java/JMX workloads Builds a dashboard to visualize Java/JMX metrics in an existing Amazon Managed Grafana workspace specified in the earlier step and configures the Amazon Managed Service for Prometheus workspace as a data source After provisioning the EKS cluster, you\u2019ll add the Amazon EKS Cluster endpoint to the kubeconfig and verify if the resources provision successfully. aws eks --region $AWS_REGION update-kubeconfig --name aws001-preprod-dev-eks Verify the creation of Amazon Managed Service for Prometheus workspace and ADOT collector by running the following command: aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"amp-ws-aws001-preprod-dev-eks\").workspaceId' Listing all the pods from the cluster kubectl get pods -A We should be able to verify the connection between Amazon Managed Grafana and Amazon Managed Prometheus by heading to the configuration page and looking at the default data source. Select the Data source named amp Scroll down and select Save & test It should display a success message like below","title":"Step 4: Deploying the Terraform modules"},{"location":"recipes/eks-observability-accelerator/#step-5-deploying-sample-javajmx-application","text":"You\u2019ll deploy a sample Java/JMX application and start[nn1] scraping the JMX metrics. The sample application generates JMX metrics, such as the JVM memory pool, JVM memory usage, and thread, and exports it in the Prometheus format. You\u2019ll deploy a load generator and a bad load generator to get a wide range of metrics to visualize eventually. The EKS Observability accelerator collects the metrics for the AWS OTEL operator deployment. ADOT exporter will ingest these metrics into the Amazon Managed Service for Prometheus workspace. We\u2019ll reuse an example from the AWS OpenTelemetry collector repository : # Switch to home directory cd ~/ #Clone the git repository git clone https://github.com/aws-observability/aws-otel-test-framework.git #Setup environment variables export AWS_ACCOUNT_ID=`aws sts get-caller-identity --query Account --output text` #Login to registry aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com #Create ECR Repository aws ecr create-repository --repository-name prometheus-sample-tomcat-jmx \\ --image-scanning-configuration scanOnPush=true \\ --region $AWS_REGION #Build Docker image and push to ECR cd ~/aws-otel-test-framework/sample-apps/jmx docker build -t $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest . docker push $AWS_ACCOUNT_ID.dkr.ecr.$AWS_REGION.amazonaws.com/prometheus-sample-tomcat-jmx:latest #Deploy the sample application export SAMPLE_TRAFFIC_NAMESPACE=javajmx-sample curl https://raw.githubusercontent.com/aws-observability/aws-otel-test-framework/terraform/sample-apps/jmx/examples/prometheus-metrics-sample.yaml > metrics-sample.yaml sed -e \"s/{{aws_account_id}}/$AWS_ACCOUNT_ID/g\" metrics-sample.yaml -i sed -e \"s/{{region}}/$AWS_REGION/g\" metrics-sample.yaml -i sed -e \"s/{{namespace}}/$SAMPLE_TRAFFIC_NAMESPACE/g\" metrics-sample.yaml -i kubectl apply -f metrics-sample.yaml #Verify the application kubectl get pods -n $SAMPLE_TRAFFIC_NAMESPACE","title":"Step 5: Deploying sample Java/JMX application"},{"location":"recipes/eks-observability-accelerator/#step-6-visualize-the-jmx-metrics-on-amazon-managed-grafana","text":"To visualize the JMX metrics collected by the AWS ADOT operator, log in to the Grafana workspace. Select Dashboards and choose Manage Select the Observability folder and choose the dashboard named EKS Accelerator \u2013 Observability \u2013 Java/JMX The Terraform module added the Amazon Managed Service for Prometheus workspace as the default data source, and created a custom dashboard to visualize the metrics. You can also deploy sample applications for NGINX, HAProxy, and Memcached.","title":"Step 6: Visualize the JMX metrics on Amazon Managed Grafana"},{"location":"recipes/eks-observability-accelerator/#clean-up","text":"Run the following command to tear down the resources provisioned by the Terraform module: terraform destroy -var-file=./dev.tfvars -auto-approve","title":"Clean up"},{"location":"recipes/eks-observability-accelerator/#conclusion","text":"Customers can now leverage EKS Observability Accelerator to deploy the opinionated EKS clusters and configure observability for specific workloads without spending much time manually deploying the resources and configuring the agent to scrape the metrics. Furthermore, the solution provides the extensibility to connect the Amazon Managed Prometheus workspace with Amazon Managed Grafana and configure alerts and notifications.","title":"Conclusion"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/","text":"Using AWS Distro for OpenTelemetry in EKS on Fargate with Amazon Managed Service for Prometheus \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus . Then we're using Amazon Managed Grafana to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus. The ADOT Collector includes two components specific to Prometheus: the Prometheus Receiver, and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP . Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment. Create EKS on Fargate cluster \u00b6 Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster-config.yaml template file by changing <YOUR_REGION> to one of the supported regions for AMP . Make sure to set <YOUR_REGION> in your shell session, for example, in Bash: export AWS_DEFAULT_REGION=<YOUR_REGION> Create your cluster using the following command: eksctl create cluster -f cluster-config.yaml Create ECR repository \u00b6 In order to deploy our application to EKS we need a container repository. You can use the following command to create a new ECR repository in your account. Make sure to set <YOUR_REGION> as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION> Set up AMP \u00b6 First, create an Amazon Managed Service for Prometheus workspace using the AWS CLI with: aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide. Set up ADOT Collector \u00b6 Download adot-collector-fargate.yaml and edit this YAML doc with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <YOUR_REGION> with your current region. 2. Replace <YOUR_ENDPOINT> with the remote write URL of your workspace. Get your AMP remote write URL endpoint by executing the following queries. First, get the workspace ID like so: YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\ --alias prometheus-sample-app \\ --query 'workspaces[0].workspaceId' --output text) Now get the remote write URL endpoint URL for your workspace using: YOUR_ENDPOINT=$(aws amp describe-workspace \\ --workspace-id $YOUR_WORKSPACE_ID \\ --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write Warning Make sure that YOUR_ENDPOINT is in fact the remote write URL, that is, the URL should end in /api/v1/remote_write . After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f adot-collector-fargate.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup . Set up AMG \u00b6 Set up a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation. Application \u00b6 In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint. Build container image \u00b6 To build the container image, first clone the Git repository and change into the directory as follows: git clone https://github.com/aws-observability/aws-otel-community.git && \\ cd ./aws-otel-community/sample-apps/prometheus First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <YOUR_REGION> with your current region. For example, in the Bash shell this would look as follows: export AWS_DEFAULT_REGION=<YOUR_REGION> export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Next, build the container image: docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Now you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Deploy sample app \u00b6 Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and AWS_DEFAULT_REGION in the file with your own values: # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Now you can deploy the sample app to your cluster using: kubectl apply -f prometheus-sample-app.yaml End-to-end \u00b6 Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG. Verify your pipeline is working \u00b6 To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs. Enter the following command to follow the ADOT collector logs: kubectl -n adot-col logs adot-collector -f One example output in the logs of the scraped metrics from the sample app should look like the following: ... Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 ... Tip To verify if AMP received the metrics, you can use awscurl . This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace $AMP_ENDPOINT with the endpoint for your AMP workspace: $ awscurl --service=\"aps\" \\ --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}} Create a Grafana dashboard \u00b6 You can import an example dashboard, available via prometheus-sample-app-dashboard.json , for the sample app that looks as follows: Further, use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on Fargate to ingest metrics. Cleanup \u00b6 First remove the Kubernetes resources and destroy the EKS cluster: kubectl delete all --all && \\ eksctl delete cluster --name amp-eks-fargate Remove the Amazon Managed Service for Prometheus workspace: aws amp delete-workspace --workspace-id \\ `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the IAM role: aws delete-role --role-name adot-collector-role Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console.","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with Amazon Managed Service for Prometheus"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#using-aws-distro-for-opentelemetry-in-eks-on-fargate-with-amazon-managed-service-for-prometheus","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest metrics into Amazon Managed Service for Prometheus . Then we're using Amazon Managed Grafana to visualize the metrics. We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete.","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with Amazon Managed Service for Prometheus"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#architecture","text":"The ADOT pipeline enables us to use the ADOT Collector to scrape a Prometheus-instrumented application, and ingest the scraped metrics to Amazon Managed Service for Prometheus. The ADOT Collector includes two components specific to Prometheus: the Prometheus Receiver, and the AWS Prometheus Remote Write Exporter. Info For more information on Prometheus Remote Write Exporter check out: Getting Started with Prometheus Remote Write Exporter for AMP .","title":"Architecture"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment.","title":"Prerequisites"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#create-eks-on-fargate-cluster","text":"Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster-config.yaml template file by changing <YOUR_REGION> to one of the supported regions for AMP . Make sure to set <YOUR_REGION> in your shell session, for example, in Bash: export AWS_DEFAULT_REGION=<YOUR_REGION> Create your cluster using the following command: eksctl create cluster -f cluster-config.yaml","title":"Create EKS on Fargate cluster"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#create-ecr-repository","text":"In order to deploy our application to EKS we need a container repository. You can use the following command to create a new ECR repository in your account. Make sure to set <YOUR_REGION> as well. aws ecr create-repository \\ --repository-name prometheus-sample-app \\ --image-scanning-configuration scanOnPush=true \\ --region <YOUR_REGION>","title":"Create ECR repository"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amp","text":"First, create an Amazon Managed Service for Prometheus workspace using the AWS CLI with: aws amp create-workspace --alias prometheus-sample-app Verify the workspace is created using: aws amp list-workspaces Info For more details check out the AMP Getting started guide.","title":"Set up AMP"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-adot-collector","text":"Download adot-collector-fargate.yaml and edit this YAML doc with the parameters described in the next steps. In this example, the ADOT Collector configuration uses an annotation (scrape=true) to tell which target endpoints to scrape. This allows the ADOT Collector to distinguish the sample app endpoint from kube-system endpoints in your cluster. You can remove this from the re-label configurations if you want to scrape a different sample app. Use the following steps to edit the downloaded file for your environment: 1. Replace <YOUR_REGION> with your current region. 2. Replace <YOUR_ENDPOINT> with the remote write URL of your workspace. Get your AMP remote write URL endpoint by executing the following queries. First, get the workspace ID like so: YOUR_WORKSPACE_ID=$(aws amp list-workspaces \\ --alias prometheus-sample-app \\ --query 'workspaces[0].workspaceId' --output text) Now get the remote write URL endpoint URL for your workspace using: YOUR_ENDPOINT=$(aws amp describe-workspace \\ --workspace-id $YOUR_WORKSPACE_ID \\ --query 'workspace.prometheusEndpoint' --output text)api/v1/remote_write Warning Make sure that YOUR_ENDPOINT is in fact the remote write URL, that is, the URL should end in /api/v1/remote_write . After creating deployment file we can now apply this to our cluster by using the following command: kubectl apply -f adot-collector-fargate.yaml Info For more information check out the AWS Distro for OpenTelemetry (ADOT) Collector Setup .","title":"Set up ADOT Collector"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#set-up-amg","text":"Set up a new AMG workspace using the Amazon Managed Grafana \u2013 Getting Started guide. Make sure to add \"Amazon Managed Service for Prometheus\" as a datasource during creation.","title":"Set up AMG"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#application","text":"In this recipe we will be using a sample application from the AWS Observability repository. This Prometheus sample app generates all four Prometheus metric types (counter, gauge, histogram, summary) and exposes them at the /metrics endpoint.","title":"Application"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#build-container-image","text":"To build the container image, first clone the Git repository and change into the directory as follows: git clone https://github.com/aws-observability/aws-otel-community.git && \\ cd ./aws-otel-community/sample-apps/prometheus First, set the region (if not already done above) and account ID to what is applicable in your case. Replace <YOUR_REGION> with your current region. For example, in the Bash shell this would look as follows: export AWS_DEFAULT_REGION=<YOUR_REGION> export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` Next, build the container image: docker build . -t \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Note If go mod fails in your environment due to a proxy.golang.or i/o timeout, you are able to bypass the go mod proxy by editing the Dockerfile. Change the following line in the Docker file: RUN GO111MODULE=on go mod download to: RUN GOPROXY=direct GO111MODULE=on go mod download Now you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $AWS_DEFAULT_REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\"","title":"Build container image"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#deploy-sample-app","text":"Edit prometheus-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and AWS_DEFAULT_REGION in the file with your own values: # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.AWS_DEFAULT_REGION.amazonaws.com/prometheus-sample-app:latest\" Now you can deploy the sample app to your cluster using: kubectl apply -f prometheus-sample-app.yaml","title":"Deploy sample app"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#end-to-end","text":"Now that you have the infrastructure and the application in place, we will test out the setup, sending metrics from the Go app running in EKS to AMP and visualize it in AMG.","title":"End-to-end"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#verify-your-pipeline-is-working","text":"To verify if the ADOT collector is scraping the pod of the sample app and ingests the metrics into AMP, we look at the collector logs. Enter the following command to follow the ADOT collector logs: kubectl -n adot-col logs adot-collector -f One example output in the logs of the scraped metrics from the sample app should look like the following: ... Resource labels: -> service.name: STRING(kubernetes-service-endpoints) -> host.name: STRING(192.168.16.238) -> port: STRING(8080) -> scheme: STRING(http) InstrumentationLibraryMetrics #0 Metric #0 Descriptor: -> Name: test_gauge0 -> Description: This is my gauge -> Unit: -> DataType: DoubleGauge DoubleDataPoints #0 StartTime: 0 Timestamp: 1606511460471000000 Value: 0.000000 ... Tip To verify if AMP received the metrics, you can use awscurl . This tool enables you to send HTTP requests from the command line with AWS Sigv4 authentication, so you must have AWS credentials set up locally with the correct permissions to query from AMP. In the following command replace $AMP_ENDPOINT with the endpoint for your AMP workspace: $ awscurl --service=\"aps\" \\ --region=\"$AWS_DEFAULT_REGION\" \"https://$AMP_ENDPOINT/api/v1/query?query=adot_test_gauge0\" {\"status\":\"success\",\"data\":{\"resultType\":\"vector\",\"result\":[{\"metric\":{\"__name__\":\"adot_test_gauge0\"},\"value\":[1606512592.493,\"16.87214000011479\"]}]}}","title":"Verify your pipeline is working"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#create-a-grafana-dashboard","text":"You can import an example dashboard, available via prometheus-sample-app-dashboard.json , for the sample app that looks as follows: Further, use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on Fargate to ingest metrics.","title":"Create a Grafana dashboard"},{"location":"recipes/fargate-eks-metrics-go-adot-ampamg/#cleanup","text":"First remove the Kubernetes resources and destroy the EKS cluster: kubectl delete all --all && \\ eksctl delete cluster --name amp-eks-fargate Remove the Amazon Managed Service for Prometheus workspace: aws amp delete-workspace --workspace-id \\ `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the IAM role: aws delete-role --role-name adot-collector-role Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console.","title":"Cleanup"},{"location":"recipes/fargate-eks-xray-go-adot-amg/","text":"Using AWS Distro for OpenTelemetry in EKS on Fargate with AWS X-Ray \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest traces into AWS X-Ray and visualize the traces in Amazon Managed Grafana . We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The ADOT pipeline enables us to use the ADOT Collector to collect traces from an instrumented app and ingest them into X-Ray: Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment. You have the aws-observability/aws-o11y-recipes repo cloned into your local environment. Create EKS on Fargate cluster \u00b6 Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster_config.yaml . Create your cluster using the following command: eksctl create cluster -f cluster-config.yaml Create ECR repository \u00b6 In order to deploy our application to EKS we need a container repository. We will use the private ECR registry, but you can also use ECR Public, if you want to share the container image. First, set the environment variables, such as shown here (substitute for your region): export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` You can use the following command to create a new ECR repository in your account: aws ecr create-repository \\ --repository-name ho11y \\ --image-scanning-configuration scanOnPush=true \\ --region $REGION Set up ADOT Collector \u00b6 Download adot-collector-fargate.yaml and edit this YAML doc with the parameters described in the next steps. kubectl apply -f adot-collector-fargate.yaml Set up Managed Grafana \u00b6 Set up a new workspace using the Amazon Managed Grafana \u2013 Getting Started guide and add X-Ray as a data source . Signal generator \u00b6 We will be using ho11y , a synthetic signal generator available via the sandbox of the recipes repository. So, if you haven't cloned the repo into your local environment, do now: git clone https://github.com/aws-observability/aws-o11y-recipes.git Build container image \u00b6 Make sure that your ACCOUNTID and REGION environment variables are set, for example: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` To build the ho11y container image, first change into the ./sandbox/ho11y/ directory and build the container image : Note The following build step assumes that the Docker daemon or an equivalent OCI image build tool is running. docker build . -t \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\" Push container image \u00b6 Next, you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\" Deploy signal generator \u00b6 Edit x-ray-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and REGION in the file with your own values (overall, in three locations): # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.REGION.amazonaws.com/ho11y:latest\" Now you can deploy the sample app to your cluster using: kubectl -n example-app apply -f x-ray-sample-app.yaml End-to-end \u00b6 Now that you have the infrastructure and the application in place, we will test out the setup, sending traces from ho11y running in EKS to X-Ray and visualize it in AMG. Verify pipeline \u00b6 To verify if the ADOT collector is ingesting traces from ho11y , we make one of the services available locally and invoke it. First, let's forward traffic as so: kubectl -n example-app port-forward svc/frontend 8765:80 With above command, the frontend microservice (a ho11y instance configured to talk to two other ho11y instances) is available in your local environment and you can invoke it as follows (triggering the creation of traces): $ curl localhost:8765/ {\"traceId\":\"1-6193a9be-53693f29a0119ee4d661ba0d\"} Tip If you want to automate the invocation, you can wrap the curl call into a while true loop. To verify our setup, visit the X-Ray view in CloudWatch where you should see something like shown below: Now that we have the signal generator set up and active and the OpenTelemetry pipeline set up, let's see how to consume the traces in Grafana. Grafana dashboard \u00b6 You can import an example dashboard, available via x-ray-sample-dashboard.json that looks as follows: Further, when you click on any of the traces in the lower downstreams panel, you can dive into it and view it in the \"Explore\" tab like so: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on Fargate to ingest traces. Cleanup \u00b6 First remove the Kubernetes resources and destroy the EKS cluster: kubectl delete all --all && \\ eksctl delete cluster --name xray-eks-fargate Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console.","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with AWS X-Ray"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#using-aws-distro-for-opentelemetry-in-eks-on-fargate-with-aws-x-ray","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a sample Go application and use AWS Distro for OpenTelemetry (ADOT) to ingest traces into AWS X-Ray and visualize the traces in Amazon Managed Grafana . We will be setting up an Amazon Elastic Kubernetes Service (EKS) on AWS Fargate cluster and use an Amazon Elastic Container Registry (ECR) repository to demonstrate a complete scenario. Note This guide will take approximately 1 hour to complete.","title":"Using AWS Distro for OpenTelemetry in EKS on Fargate with AWS X-Ray"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#architecture","text":"The ADOT pipeline enables us to use the ADOT Collector to collect traces from an instrumented app and ingest them into X-Ray:","title":"Architecture"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment. You have the aws-observability/aws-o11y-recipes repo cloned into your local environment.","title":"Prerequisites"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#create-eks-on-fargate-cluster","text":"Our demo application is a Kubernetes app that we will run in an EKS on Fargate cluster. So, first create an EKS cluster using the provided cluster_config.yaml . Create your cluster using the following command: eksctl create cluster -f cluster-config.yaml","title":"Create EKS on Fargate cluster"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#create-ecr-repository","text":"In order to deploy our application to EKS we need a container repository. We will use the private ECR registry, but you can also use ECR Public, if you want to share the container image. First, set the environment variables, such as shown here (substitute for your region): export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` You can use the following command to create a new ECR repository in your account: aws ecr create-repository \\ --repository-name ho11y \\ --image-scanning-configuration scanOnPush=true \\ --region $REGION","title":"Create ECR repository"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#set-up-adot-collector","text":"Download adot-collector-fargate.yaml and edit this YAML doc with the parameters described in the next steps. kubectl apply -f adot-collector-fargate.yaml","title":"Set up ADOT Collector"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#set-up-managed-grafana","text":"Set up a new workspace using the Amazon Managed Grafana \u2013 Getting Started guide and add X-Ray as a data source .","title":"Set up Managed Grafana"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#signal-generator","text":"We will be using ho11y , a synthetic signal generator available via the sandbox of the recipes repository. So, if you haven't cloned the repo into your local environment, do now: git clone https://github.com/aws-observability/aws-o11y-recipes.git","title":"Signal generator"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#build-container-image","text":"Make sure that your ACCOUNTID and REGION environment variables are set, for example: export REGION=\"eu-west-1\" export ACCOUNTID=`aws sts get-caller-identity --query Account --output text` To build the ho11y container image, first change into the ./sandbox/ho11y/ directory and build the container image : Note The following build step assumes that the Docker daemon or an equivalent OCI image build tool is running. docker build . -t \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"","title":"Build container image"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#push-container-image","text":"Next, you can push the container image to the ECR repo you created earlier on. For that, first log in to the default ECR registry: aws ecr get-login-password --region $REGION | \\ docker login --username AWS --password-stdin \\ \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com\" And finally, push the container image to the ECR repository you created, above: docker push \"$ACCOUNTID.dkr.ecr.$REGION.amazonaws.com/ho11y:latest\"","title":"Push container image"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#deploy-signal-generator","text":"Edit x-ray-sample-app.yaml to contain your ECR image path. That is, replace ACCOUNTID and REGION in the file with your own values (overall, in three locations): # change the following to your container image: image: \"ACCOUNTID.dkr.ecr.REGION.amazonaws.com/ho11y:latest\" Now you can deploy the sample app to your cluster using: kubectl -n example-app apply -f x-ray-sample-app.yaml","title":"Deploy signal generator"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#end-to-end","text":"Now that you have the infrastructure and the application in place, we will test out the setup, sending traces from ho11y running in EKS to X-Ray and visualize it in AMG.","title":"End-to-end"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#verify-pipeline","text":"To verify if the ADOT collector is ingesting traces from ho11y , we make one of the services available locally and invoke it. First, let's forward traffic as so: kubectl -n example-app port-forward svc/frontend 8765:80 With above command, the frontend microservice (a ho11y instance configured to talk to two other ho11y instances) is available in your local environment and you can invoke it as follows (triggering the creation of traces): $ curl localhost:8765/ {\"traceId\":\"1-6193a9be-53693f29a0119ee4d661ba0d\"} Tip If you want to automate the invocation, you can wrap the curl call into a while true loop. To verify our setup, visit the X-Ray view in CloudWatch where you should see something like shown below: Now that we have the signal generator set up and active and the OpenTelemetry pipeline set up, let's see how to consume the traces in Grafana.","title":"Verify pipeline"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#grafana-dashboard","text":"You can import an example dashboard, available via x-ray-sample-dashboard.json that looks as follows: Further, when you click on any of the traces in the lower downstreams panel, you can dive into it and view it in the \"Explore\" tab like so: From here, you can use the following guides to create your own dashboard in Amazon Managed Grafana: User Guide: Dashboards Best practices for creating dashboards That's it, congratulations you've learned how to use ADOT in EKS on Fargate to ingest traces.","title":"Grafana dashboard"},{"location":"recipes/fargate-eks-xray-go-adot-amg/#cleanup","text":"First remove the Kubernetes resources and destroy the EKS cluster: kubectl delete all --all && \\ eksctl delete cluster --name xray-eks-fargate Finally, remove the Amazon Managed Grafana workspace by removing it via the AWS console.","title":"Cleanup"},{"location":"recipes/lambda-cw-metrics-go-amp/","text":"Exporting CloudWatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a CloudWatch Metric Stream and use Kinesis Data Firehose and AWS Lambda to ingest metrics into Amazon Managed Service for Prometheus (AMP) . We will be setting up a stack using AWS Cloud Development Kit (CDK) to create a Firehose Delivery Stream, Lambda, and a S3 Bucket to demonstrate a complete scenario. Note This guide will take approximately 30 minutes to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. CloudWatch Metric Streams allow forwarding of the streaming metric data to a HTTP endpoint or S3 bucket . Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. The AWS CDK Typescript is installed in your environment. Node.js and Go. The repo has been cloned to your local machine. Create an AMP workspace \u00b6 Our demo application in this recipe will be running on top of AMP. Create your AMP Workspace via the following command: aws amp create-workspace --alias prometheus-demo-recipe Ensure your workspace has been created with the following command: aws amp list-workspaces Info For more details check out the AMP Getting started guide. Install dependencies \u00b6 From the root of the aws-o11y-recipes repository, change your directory to CWMetricStreamExporter via the command: cd sandbox/CWMetricStreamExporter This will now be considered the root of the repo, going forward. Change directory to /cdk via the following command: cd cdk Install the CDK dependencies via the following command: npm install Change directory back to the root of the repo, and then change directory to /lambda using following command: cd lambda Once in the /lambda folder, install the Go dependencies using: go get All the dependencies are now installed. Modify config file \u00b6 In the root of the repo, open config.yaml and modify the AMP workspace URL by replacing the {workspace} with the newly created workspace id, and the region your AMP workspace is in. For example, modify the following with: AMP: remote_write_url: \"https://aps-workspaces.us-east-2.amazonaws.com/workspaces/{workspaceId}/api/v1/remote_write\" region: us-east-2 Change the names of the Firehose Delivery Stream and S3 Bucket to your liking. Deploy stack \u00b6 Once the config.yaml has been modified with the AMP workspace ID, it is time to deploy the stack to CloudFormation. To build the CDK and the Lambda code, in the root of the repo run the following commend: npm run build This build step ensures that the Go Lambda binary is built, and deploys the CDK to CloudFormation. Accept the following IAM changes to deploy the stack: Verify that the stack has been created by running the following command: aws cloudformation list-stacks A stack by the name CDK Stack should have been created. Create CloudWatch stream \u00b6 Navigate to the CloudWatch consoloe, for example https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metric-streams:streamsList and click \"Create metric stream\". Select the metics needed, either all metrics of only from selected namespaces. Configure the Metric Stream by using an existing Firehose which was created by the CDK. Change the output format to JSON instead of OpenTelemetry 0.7. Modify the Metric Stream name to your liking, and click \"Create metric stream\": To verify the Lambda function invocation, navigate to the Lambda console and click the function KinesisMessageHandler . Click the Monitor tab and Logs subtab, and under Recent Invocations there should be entries of the Lambda function being triggered. Note It may take upto 5 minutes for invocations to show in the Monitor tab. That is it! Congratulations, your metrics are now being streamed from CloudWatch to Amazon Managed Service for Prometheus. Cleanup \u00b6 First, delete the CloudFormation stack: cd cdk cdk destroy Remove the AMP workspace: aws amp delete-workspace --workspace-id \\ `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Last but not least, remove the CloudWatch Metric Stream by removing it from the console.","title":"Exporting CloudWatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus"},{"location":"recipes/lambda-cw-metrics-go-amp/#exporting-cloudwatch-metric-streams-via-firehose-and-aws-lambda-to-amazon-managed-service-for-prometheus","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to instrument a CloudWatch Metric Stream and use Kinesis Data Firehose and AWS Lambda to ingest metrics into Amazon Managed Service for Prometheus (AMP) . We will be setting up a stack using AWS Cloud Development Kit (CDK) to create a Firehose Delivery Stream, Lambda, and a S3 Bucket to demonstrate a complete scenario. Note This guide will take approximately 30 minutes to complete.","title":"Exporting CloudWatch Metric Streams via Firehose and AWS Lambda to Amazon Managed Service for Prometheus"},{"location":"recipes/lambda-cw-metrics-go-amp/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe. CloudWatch Metric Streams allow forwarding of the streaming metric data to a HTTP endpoint or S3 bucket .","title":"Infrastructure"},{"location":"recipes/lambda-cw-metrics-go-amp/#prerequisites","text":"The AWS CLI is installed and configured in your environment. The AWS CDK Typescript is installed in your environment. Node.js and Go. The repo has been cloned to your local machine.","title":"Prerequisites"},{"location":"recipes/lambda-cw-metrics-go-amp/#create-an-amp-workspace","text":"Our demo application in this recipe will be running on top of AMP. Create your AMP Workspace via the following command: aws amp create-workspace --alias prometheus-demo-recipe Ensure your workspace has been created with the following command: aws amp list-workspaces Info For more details check out the AMP Getting started guide.","title":"Create an AMP workspace"},{"location":"recipes/lambda-cw-metrics-go-amp/#install-dependencies","text":"From the root of the aws-o11y-recipes repository, change your directory to CWMetricStreamExporter via the command: cd sandbox/CWMetricStreamExporter This will now be considered the root of the repo, going forward. Change directory to /cdk via the following command: cd cdk Install the CDK dependencies via the following command: npm install Change directory back to the root of the repo, and then change directory to /lambda using following command: cd lambda Once in the /lambda folder, install the Go dependencies using: go get All the dependencies are now installed.","title":"Install dependencies"},{"location":"recipes/lambda-cw-metrics-go-amp/#modify-config-file","text":"In the root of the repo, open config.yaml and modify the AMP workspace URL by replacing the {workspace} with the newly created workspace id, and the region your AMP workspace is in. For example, modify the following with: AMP: remote_write_url: \"https://aps-workspaces.us-east-2.amazonaws.com/workspaces/{workspaceId}/api/v1/remote_write\" region: us-east-2 Change the names of the Firehose Delivery Stream and S3 Bucket to your liking.","title":"Modify config file"},{"location":"recipes/lambda-cw-metrics-go-amp/#deploy-stack","text":"Once the config.yaml has been modified with the AMP workspace ID, it is time to deploy the stack to CloudFormation. To build the CDK and the Lambda code, in the root of the repo run the following commend: npm run build This build step ensures that the Go Lambda binary is built, and deploys the CDK to CloudFormation. Accept the following IAM changes to deploy the stack: Verify that the stack has been created by running the following command: aws cloudformation list-stacks A stack by the name CDK Stack should have been created.","title":"Deploy stack"},{"location":"recipes/lambda-cw-metrics-go-amp/#create-cloudwatch-stream","text":"Navigate to the CloudWatch consoloe, for example https://console.aws.amazon.com/cloudwatch/home?region=us-east-1#metric-streams:streamsList and click \"Create metric stream\". Select the metics needed, either all metrics of only from selected namespaces. Configure the Metric Stream by using an existing Firehose which was created by the CDK. Change the output format to JSON instead of OpenTelemetry 0.7. Modify the Metric Stream name to your liking, and click \"Create metric stream\": To verify the Lambda function invocation, navigate to the Lambda console and click the function KinesisMessageHandler . Click the Monitor tab and Logs subtab, and under Recent Invocations there should be entries of the Lambda function being triggered. Note It may take upto 5 minutes for invocations to show in the Monitor tab. That is it! Congratulations, your metrics are now being streamed from CloudWatch to Amazon Managed Service for Prometheus.","title":"Create CloudWatch stream"},{"location":"recipes/lambda-cw-metrics-go-amp/#cleanup","text":"First, delete the CloudFormation stack: cd cdk cdk destroy Remove the AMP workspace: aws amp delete-workspace --workspace-id \\ `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Last but not least, remove the CloudWatch Metric Stream by removing it from the console.","title":"Cleanup"},{"location":"recipes/metrics-explorer-filter-by-tags/","text":"Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Metrics explorer to filter, aggregate, and visualize metrics by resource tags and resource properties - Use metrics explorer to monitor resources by their tags and properties . There are number of ways to create visualizations with Metrics explorer; in this walkthrough we simply leverage the AWS Console. Note This guide will take approximately 5 minutes to complete. Prerequisites \u00b6 Access to AWS account Access to Amazon CloudWatch Metrics explorer via AWS Console Resource tags set for the relevant resources Metrics Explorer tag based queries and visualizations \u00b6 Open the CloudWatch console Under Metrics , click on the Explorer menu You can either choose from one of the Generic templates or a Service based templates list; in this example we use the EC2 Instances by type template Choose metrics you would like to explore; remove obsolete once, and add other metrics you would like to see Under From , choose a resource tag or a resource property you are looking for; in the below example we show number of CPU and Network related metrics for different EC2 instances with Name: TeamX Tag Please note, you can combine time series using an aggregation function under Aggregated by ; in the below example TeamX metrics are aggregated by Availability Zone Alternatively, you could aggregate TeamX and TeamY by the Team Tag, or choose any other configuration that suits your needs Dynamic visualizations \u00b6 You can easily customize resulting visualizations by using From , Aggregated by and Split by options. Metrics explorer visualizations are dynamic, so any new tagged resource automatically appears in the explorer widget. Reference \u00b6 For more information on Metrics explorer please refer to the following article: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metrics-Explorer.html","title":"Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags"},{"location":"recipes/metrics-explorer-filter-by-tags/#using-amazon-cloudwatch-metrics-explorer-to-aggregate-and-visualize-metrics-filtered-by-resource-tags","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to use Metrics explorer to filter, aggregate, and visualize metrics by resource tags and resource properties - Use metrics explorer to monitor resources by their tags and properties . There are number of ways to create visualizations with Metrics explorer; in this walkthrough we simply leverage the AWS Console. Note This guide will take approximately 5 minutes to complete.","title":"Using Amazon CloudWatch Metrics explorer to aggregate and visualize metrics filtered by resource tags"},{"location":"recipes/metrics-explorer-filter-by-tags/#prerequisites","text":"Access to AWS account Access to Amazon CloudWatch Metrics explorer via AWS Console Resource tags set for the relevant resources","title":"Prerequisites"},{"location":"recipes/metrics-explorer-filter-by-tags/#metrics-explorer-tag-based-queries-and-visualizations","text":"Open the CloudWatch console Under Metrics , click on the Explorer menu You can either choose from one of the Generic templates or a Service based templates list; in this example we use the EC2 Instances by type template Choose metrics you would like to explore; remove obsolete once, and add other metrics you would like to see Under From , choose a resource tag or a resource property you are looking for; in the below example we show number of CPU and Network related metrics for different EC2 instances with Name: TeamX Tag Please note, you can combine time series using an aggregation function under Aggregated by ; in the below example TeamX metrics are aggregated by Availability Zone Alternatively, you could aggregate TeamX and TeamY by the Team Tag, or choose any other configuration that suits your needs","title":"Metrics Explorer tag based queries and visualizations"},{"location":"recipes/metrics-explorer-filter-by-tags/#dynamic-visualizations","text":"You can easily customize resulting visualizations by using From , Aggregated by and Split by options. Metrics explorer visualizations are dynamic, so any new tagged resource automatically appears in the explorer widget.","title":"Dynamic visualizations"},{"location":"recipes/metrics-explorer-filter-by-tags/#reference","text":"For more information on Metrics explorer please refer to the following article: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/CloudWatch-Metrics-Explorer.html","title":"Reference"},{"location":"recipes/monitoring-hybridenv-amg/","text":"Monitoring hybrid environments using Amazon Managed Service for Grafana \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to visualize metrics from an Azure Cloud environment to Amazon Managed Service for Grafana (AMG) and create alert notifications in AMG to be sent to Amazon Simple Notification Service and Slack. As part of the implementation, we will create an AMG workspace, configure the Azure Monitor plugin as the data source for AMG and configure the Grafana dashboard. We will be creating two notification channels: one for Amazon SNS and one for slack.We will also configure alerts in the AMG dashboard to be sent to the notification channels. Note This guide will take approximately 30 minutes to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to enable AWS-SSO Architecture \u00b6 First, create an AMG workspace to visualize the metrics from Azure Monitor. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Note You must assign an Admin role to at least one user in the workspace. In Figure 1, the user name is grafana-admin. The user type is Admin. On the Data sources tab, choose the required data source. Review the configuration, and then choose Create workspace. Configure the data source and custom dashboard \u00b6 Now, under Data sources, configure the Azure Monitor plugin to start querying and visualizing the metrics from the Azure environment. Choose Data sources to add a data source. In Add data source, search for Azure Monitor and then configure the parameters from the app registration console in the Azure environment. To configure the Azure Monitor plugin, you need the directory (tenant) ID and the application (client) ID. For instructions, see the article about creating an Azure AD application and service principal. It explains how to register the app and grant access to Grafana to query the data. After the data source is configured, import a custom dashboard to analyze the Azure metrics. In the left pane, choose the + icon, and then choose Import. In Import via grafana.com, enter the dashboard ID, 10532. This will import the Azure Virtual Machine dashboard where you can start analyzing the Azure Monitor metrics. In my setup, I have a virtual machine running in the Azure environment. Configure the notification channels on AMG \u00b6 In this section, you\u2019ll configure two notifications channels and then send alerts. Use the following command to create an SNS topic named grafana-notification and subscribe an email address. aws sns create-topic --name grafana-notification aws sns subscribe --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification --protocol email --notification-endpoint <email-id> In the left pane, choose the bell icon to add a new notification channel. Now configure the grafana-notification notification channel. On Edit notification channel, for Type, choose AWS SNS. For Topic, use the ARN of the SNS topic you just created. For Auth Provider, choose the workspace IAM role. Slack notification channel \u00b6 To configure a Slack notification channel, create a Slack workspace or use an existing one. Enable Incoming Webhooks as described in Sending messages using Incoming Webhooks . After you\u2019ve configured the workspace, you should be able to get a webhook URL that will be used in the Grafana dashboard. Configure alerts in AMG \u00b6 You can configure Grafana alerts when the metric increases beyond the threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. In this example, configure an alert for CPU utilization for an Azure virtual machine. When the utilization exceeds a threshold, configure AMG to send notifications to both channels. In the dashboard, choose CPU utilization from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the CPU utilization exceeds 50%. Notifications will be sent to the grafana-alert-notification and slack-alert-notification channels. Now, you can sign in to the Azure virtual machine and initiate stress testing using tools like stress. When the CPU utilization exceeds the threshold, you will receive notifications on both channels. Now configure alerts for CPU utilization with the right threshold to simulate an alert that is sent to the Slack channel. Conclusion \u00b6 In the recipe, we showed you how to deploy the AMG workspace, configure notification channels, collect metrics from Azure Cloud, and configure alerts on the AMG dashboard. Because AMG is a fully managed, serverless solution, you can spend your time on the applications that transform your business and leave the heavy lifting of managing Grafana to AWS.","title":"Monitoring hybrid environments using Amazon Managed Service for Grafana"},{"location":"recipes/monitoring-hybridenv-amg/#monitoring-hybrid-environments-using-amazon-managed-service-for-grafana","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to visualize metrics from an Azure Cloud environment to Amazon Managed Service for Grafana (AMG) and create alert notifications in AMG to be sent to Amazon Simple Notification Service and Slack. As part of the implementation, we will create an AMG workspace, configure the Azure Monitor plugin as the data source for AMG and configure the Grafana dashboard. We will be creating two notification channels: one for Amazon SNS and one for slack.We will also configure alerts in the AMG dashboard to be sent to the notification channels. Note This guide will take approximately 30 minutes to complete.","title":"Monitoring hybrid environments using Amazon Managed Service for Grafana"},{"location":"recipes/monitoring-hybridenv-amg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/monitoring-hybridenv-amg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to enable AWS-SSO","title":"Prerequisites"},{"location":"recipes/monitoring-hybridenv-amg/#architecture","text":"First, create an AMG workspace to visualize the metrics from Azure Monitor. Follow the steps in the Getting Started with Amazon Managed Service for Grafana blog post. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Note You must assign an Admin role to at least one user in the workspace. In Figure 1, the user name is grafana-admin. The user type is Admin. On the Data sources tab, choose the required data source. Review the configuration, and then choose Create workspace.","title":"Architecture"},{"location":"recipes/monitoring-hybridenv-amg/#configure-the-data-source-and-custom-dashboard","text":"Now, under Data sources, configure the Azure Monitor plugin to start querying and visualizing the metrics from the Azure environment. Choose Data sources to add a data source. In Add data source, search for Azure Monitor and then configure the parameters from the app registration console in the Azure environment. To configure the Azure Monitor plugin, you need the directory (tenant) ID and the application (client) ID. For instructions, see the article about creating an Azure AD application and service principal. It explains how to register the app and grant access to Grafana to query the data. After the data source is configured, import a custom dashboard to analyze the Azure metrics. In the left pane, choose the + icon, and then choose Import. In Import via grafana.com, enter the dashboard ID, 10532. This will import the Azure Virtual Machine dashboard where you can start analyzing the Azure Monitor metrics. In my setup, I have a virtual machine running in the Azure environment.","title":"Configure the data source and custom dashboard"},{"location":"recipes/monitoring-hybridenv-amg/#configure-the-notification-channels-on-amg","text":"In this section, you\u2019ll configure two notifications channels and then send alerts. Use the following command to create an SNS topic named grafana-notification and subscribe an email address. aws sns create-topic --name grafana-notification aws sns subscribe --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification --protocol email --notification-endpoint <email-id> In the left pane, choose the bell icon to add a new notification channel. Now configure the grafana-notification notification channel. On Edit notification channel, for Type, choose AWS SNS. For Topic, use the ARN of the SNS topic you just created. For Auth Provider, choose the workspace IAM role.","title":"Configure the notification channels on AMG"},{"location":"recipes/monitoring-hybridenv-amg/#slack-notification-channel","text":"To configure a Slack notification channel, create a Slack workspace or use an existing one. Enable Incoming Webhooks as described in Sending messages using Incoming Webhooks . After you\u2019ve configured the workspace, you should be able to get a webhook URL that will be used in the Grafana dashboard.","title":"Slack notification channel"},{"location":"recipes/monitoring-hybridenv-amg/#configure-alerts-in-amg","text":"You can configure Grafana alerts when the metric increases beyond the threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. In this example, configure an alert for CPU utilization for an Azure virtual machine. When the utilization exceeds a threshold, configure AMG to send notifications to both channels. In the dashboard, choose CPU utilization from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the CPU utilization exceeds 50%. Notifications will be sent to the grafana-alert-notification and slack-alert-notification channels. Now, you can sign in to the Azure virtual machine and initiate stress testing using tools like stress. When the CPU utilization exceeds the threshold, you will receive notifications on both channels. Now configure alerts for CPU utilization with the right threshold to simulate an alert that is sent to the Slack channel.","title":"Configure alerts in AMG"},{"location":"recipes/monitoring-hybridenv-amg/#conclusion","text":"In the recipe, we showed you how to deploy the AMG workspace, configure notification channels, collect metrics from Azure Cloud, and configure alerts on the AMG dashboard. Because AMG is a fully managed, serverless solution, you can spend your time on the applications that transform your business and leave the heavy lifting of managing Grafana to AWS.","title":"Conclusion"},{"location":"recipes/servicemesh-monitoring-ampamg/","text":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS \u00b6 Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to ingest App Mesh Envoy metrics in an Amazon Elastic Kubernetes Service (EKS) cluster to Amazon Managed Service for Prometheus (AMP) and create a custom dashboard on Amazon Managed Grafana (AMG) to monitor the health and performance of microservices. As part of the implementation, we will create an AMP workspace, install the App Mesh Controller for Kubernetes and inject the Envoy container into the pods. We will be collecting the Envoy metrics using Grafana Agent configured in the EKS cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard. Note This guide will take approximately 45 minutes to complete. Infrastructure \u00b6 In the following section we will be setting up the infrastructure for this recipe. Architecture \u00b6 The Grafana agent is configured to scrape the Envoy metrics and ingest them to AMP through the AMP remote write endpoint Info For more information on Prometheus Remote Write Exporter check out Getting Started with Prometheus Remote Write Exporter for AMP . Prerequisites \u00b6 The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment. You need AMP workspace configured in your AWS account. You need to install Helm . You need to enable AWS-SSO . Setup an EKS cluster \u00b6 First, create an EKS cluster that will be enabled with App Mesh for running the sample application. The eksctl CLI will be used to deploy the cluster using the eks-cluster-config.yaml . This template will create a new cluster with EKS. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example, in the Bash shell: export AWS_REGION=eu-west-1 Create your cluster using the following command: eksctl create cluster -f eks-cluster-config.yaml This creates an EKS cluster named AMP-EKS-CLUSTER and a service account named appmesh-controller that will be used by the App Mesh controller for EKS. Install App Mesh Controller \u00b6 Next, we will run the below commands to install the App Mesh Controller and configure the Custom Resource Definitions (CRDs): helm repo add eks https://aws.github.io/eks-charts helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set region=${AWS_REGION} \\ --set serviceAccount.create=false \\ --set serviceAccount.name=appmesh-controller Set up AMP \u00b6 The AMP workspace is used to ingest the Prometheus metrics collected from Envoy. A workspace is a logical Cortex server dedicated to a tenant. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, and delete, and the ingestion and querying of metrics. Create a workspace using the AWS CLI: aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION Add the necessary Helm repositories: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts && \\ helm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics For more details on AMP check out the AMP Getting started guide. Scraping & ingesting metrics \u00b6 AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster. You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the process of configuring the Grafana Agent to scrape the Envoy metrics and analyze them using AMP and AMG. Configure Grafana Agent \u00b6 The Grafana Agent is a lightweight alternative to running a full Prometheus server. It keeps the necessary parts for discovering and scraping Prometheus exporters and sending metrics to a Prometheus-compatible backend. The Grafana Agent also includes native support for AWS Signature Version 4 (Sigv4) for AWS Identity and Access Management (IAM) authentication. We now walk you through the steps to configure an IAM role to send Prometheus metrics to AMP. We install the Grafana Agent on the EKS cluster and forward metrics to AMP. Configure permissions \u00b6 The Grafana Agent scrapes operational metrics from containerized workloads running in the EKS cluster and sends them to AMP. Data sent to AMP must be signed with valid AWS credentials using Sigv4 to authenticate and authorize each client request for the managed service. The Grafana Agent can be deployed to an EKS cluster to run under the identity of a Kubernetes service account. With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. Prepare the IRSA setup as follows: kubectl create namespace grafana-agent export WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId') export ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text) export NAMESPACE=\"grafana-agent\" export REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\" You can use the gca-permissions.sh shell script to automate the following steps (note to replace the placeholder variable YOUR_EKS_CLUSTER_NAME with the name of your EKS cluster): Creates an IAM role named EKS-GrafanaAgent-AMP-ServiceAccount-Rol e with an IAM policy that has permissions to remote-write into an AMP workspace. Creates a Kubernetes service account named grafana-agent under the grafana-agent namespace that is associated with the IAM role. Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster. You need kubectl and eksctl CLI tools to run the gca-permissions.sh script. They must be configured to access your Amazon EKS cluster. Now create a manifest file, grafana-agent.yaml , with the scrape configuration to extract Envoy metrics and deploy the Grafana Agent. Note At time of writing, this solution will not work for EKS on Fargate due to the lack of support for daemon sets there. The example deploys a daemon set named grafana-agent and a deployment named grafana-agent-deployment . The grafana-agent daemon set collects metrics from pods on the cluster and the grafana-agent-deployment deployment collects metrics from services that do not live on the cluster, such as the EKS control plane. kubectl apply -f grafana-agent.yaml After the grafana-agent is deployed, it will collect the metrics and ingest them into the specified AMP workspace. Now deploy a sample application on the EKS cluster and start analyzing the metrics. Sample application \u00b6 To install an application and inject an Envoy container, we use the AppMesh controller for Kubernetes. First, install the base application by cloning the examples repo: git clone https://github.com/aws/aws-app-mesh-examples.git And now apply the resources to your cluster: kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application Check the pod status and make sure it is running: $ kubectl -n prod get all NAME READY STATUS RESTARTS AGE pod/dj-cb77484d7-gx9vk 1/1 Running 0 6m8s pod/jazz-v1-6b6b6dd4fc-xxj9s 1/1 Running 0 6m8s pod/metal-v1-584b9ccd88-kj7kf 1/1 Running 0 6m8s Next, install the App Mesh controller and meshify the deployment: kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/ kubectl rollout restart deployment -n prod dj jazz-v1 metal-v1 Now we should see two containers running in each pod: $ kubectl -n prod get all NAME READY STATUS RESTARTS AGE dj-7948b69dff-z6djf 2/2 Running 0 57s jazz-v1-7cdc4fc4fc-wzc5d 2/2 Running 0 57s metal-v1-7f499bb988-qtx7k 2/2 Running 0 57s Generate the traffic for 5 mins and we will visualize it AMG later: dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 300 ] ; do \\ kubectl exec -n prod -it $dj_pod -c dj \\ -- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; \\ done Create an AMG workspace \u00b6 To create an AMG workspace follow the steps in the Getting Started with AMG blog post. To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard. In this example, the user name is grafana-admin and the user type is Admin . Select the required data source. Review the configuration, and then choose Create workspace . Configure AMG datasource \u00b6 To configure AMP as a data source in AMG, in the Data sources section, choose Configure in Grafana , which will launch a Grafana workspace in the browser. You can also manually launch the Grafana workspace URL in the browser. As you can see from the screenshots, you can view Envoy metrics like downstream latency, connections, response code, and more. You can use the filters shown to drill down to the envoy metrics of a particular application. Configure AMG dashboard \u00b6 After the data source is configured, import a custom dashboard to analyze the Envoy metrics. For this we use a pre-defined dashboard, so choose Import (shown below), and then enter the ID 11022 . This will import the Envoy Global dashboard so you can start analyzing the Envoy metrics. Configure alerts on AMG \u00b6 You can configure Grafana alerts when the metric increases beyond the intended threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. Before you create alert rules, you must create a notification channel. In this example, configure Amazon SNS as a notification channel. The SNS topic must be prefixed with grafana for notifications to be successfully published to the topic if you use the defaults, that is, the service-managed permissions . Use the following command to create an SNS topic named grafana-notification : aws sns create-topic --name grafana-notification And subscribe to it via an email address. Make sure you specify the region and Account ID in the below command: aws sns subscribe \\ --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification \\ --protocol email \\ --notification-endpoint <email-id> Now, add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type, use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created. For Auth provider, choose AWS SDK Default. Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the downstream latency exceeds the threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic. Cleanup \u00b6 Remove the resources and cluster: kubectl delete all --all eksctl delete cluster --name AMP-EKS-CLUSTER Remove the AMP workspace: aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role: aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS"},{"location":"recipes/servicemesh-monitoring-ampamg/#using-amazon-managed-service-for-prometheus-to-monitor-app-mesh-environment-configured-on-eks","text":"Warning This site is being merged into the broader Observability Best Practices content. Please head over there for the latest updates, plus prescriptive guidance on the use of AWS observability tools. Warning This site will be kept as-is until January 2023, when it will be decommissioned. In this recipe we show you how to ingest App Mesh Envoy metrics in an Amazon Elastic Kubernetes Service (EKS) cluster to Amazon Managed Service for Prometheus (AMP) and create a custom dashboard on Amazon Managed Grafana (AMG) to monitor the health and performance of microservices. As part of the implementation, we will create an AMP workspace, install the App Mesh Controller for Kubernetes and inject the Envoy container into the pods. We will be collecting the Envoy metrics using Grafana Agent configured in the EKS cluster and write them to AMP. Finally, we will be creating an AMG workspace and configure the AMP as the datasource and create a custom dashboard. Note This guide will take approximately 45 minutes to complete.","title":"Using Amazon Managed Service for Prometheus to monitor App Mesh environment configured on EKS"},{"location":"recipes/servicemesh-monitoring-ampamg/#infrastructure","text":"In the following section we will be setting up the infrastructure for this recipe.","title":"Infrastructure"},{"location":"recipes/servicemesh-monitoring-ampamg/#architecture","text":"The Grafana agent is configured to scrape the Envoy metrics and ingest them to AMP through the AMP remote write endpoint Info For more information on Prometheus Remote Write Exporter check out Getting Started with Prometheus Remote Write Exporter for AMP .","title":"Architecture"},{"location":"recipes/servicemesh-monitoring-ampamg/#prerequisites","text":"The AWS CLI is installed and configured in your environment. You need to install the eksctl command in your environment. You need to install kubectl in your environment. You have Docker installed into your environment. You need AMP workspace configured in your AWS account. You need to install Helm . You need to enable AWS-SSO .","title":"Prerequisites"},{"location":"recipes/servicemesh-monitoring-ampamg/#setup-an-eks-cluster","text":"First, create an EKS cluster that will be enabled with App Mesh for running the sample application. The eksctl CLI will be used to deploy the cluster using the eks-cluster-config.yaml . This template will create a new cluster with EKS. Edit the template file and set your region to one of the available regions for AMP: us-east-1 us-east-2 us-west-2 eu-central-1 eu-west-1 Make sure to overwrite this region in your session, for example, in the Bash shell: export AWS_REGION=eu-west-1 Create your cluster using the following command: eksctl create cluster -f eks-cluster-config.yaml This creates an EKS cluster named AMP-EKS-CLUSTER and a service account named appmesh-controller that will be used by the App Mesh controller for EKS.","title":"Setup an EKS cluster"},{"location":"recipes/servicemesh-monitoring-ampamg/#install-app-mesh-controller","text":"Next, we will run the below commands to install the App Mesh Controller and configure the Custom Resource Definitions (CRDs): helm repo add eks https://aws.github.io/eks-charts helm upgrade -i appmesh-controller eks/appmesh-controller \\ --namespace appmesh-system \\ --set region=${AWS_REGION} \\ --set serviceAccount.create=false \\ --set serviceAccount.name=appmesh-controller","title":"Install App Mesh Controller"},{"location":"recipes/servicemesh-monitoring-ampamg/#set-up-amp","text":"The AMP workspace is used to ingest the Prometheus metrics collected from Envoy. A workspace is a logical Cortex server dedicated to a tenant. A workspace supports fine-grained access control for authorizing its management such as update, list, describe, and delete, and the ingestion and querying of metrics. Create a workspace using the AWS CLI: aws amp create-workspace --alias AMP-APPMESH --region $AWS_REGION Add the necessary Helm repositories: helm repo add prometheus-community https://prometheus-community.github.io/helm-charts && \\ helm repo add kube-state-metrics https://kubernetes.github.io/kube-state-metrics For more details on AMP check out the AMP Getting started guide.","title":"Set up AMP"},{"location":"recipes/servicemesh-monitoring-ampamg/#scraping-ingesting-metrics","text":"AMP does not directly scrape operational metrics from containerized workloads in a Kubernetes cluster. You must deploy and manage a Prometheus server or an OpenTelemetry agent such as the AWS Distro for OpenTelemetry Collector or the Grafana Agent to perform this task. In this receipe, we walk you through the process of configuring the Grafana Agent to scrape the Envoy metrics and analyze them using AMP and AMG.","title":"Scraping &amp; ingesting metrics"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-grafana-agent","text":"The Grafana Agent is a lightweight alternative to running a full Prometheus server. It keeps the necessary parts for discovering and scraping Prometheus exporters and sending metrics to a Prometheus-compatible backend. The Grafana Agent also includes native support for AWS Signature Version 4 (Sigv4) for AWS Identity and Access Management (IAM) authentication. We now walk you through the steps to configure an IAM role to send Prometheus metrics to AMP. We install the Grafana Agent on the EKS cluster and forward metrics to AMP.","title":"Configure Grafana Agent"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-permissions","text":"The Grafana Agent scrapes operational metrics from containerized workloads running in the EKS cluster and sends them to AMP. Data sent to AMP must be signed with valid AWS credentials using Sigv4 to authenticate and authorize each client request for the managed service. The Grafana Agent can be deployed to an EKS cluster to run under the identity of a Kubernetes service account. With IAM roles for service accounts (IRSA), you can associate an IAM role with a Kubernetes service account and thus provide IAM permissions to any pod that uses the service account. Prepare the IRSA setup as follows: kubectl create namespace grafana-agent export WORKSPACE=$(aws amp list-workspaces | jq -r '.workspaces[] | select(.alias==\"AMP-APPMESH\").workspaceId') export ROLE_ARN=$(aws iam get-role --role-name EKS-GrafanaAgent-AMP-ServiceAccount-Role --query Role.Arn --output text) export NAMESPACE=\"grafana-agent\" export REMOTE_WRITE_URL=\"https://aps-workspaces.$AWS_REGION.amazonaws.com/workspaces/$WORKSPACE/api/v1/remote_write\" You can use the gca-permissions.sh shell script to automate the following steps (note to replace the placeholder variable YOUR_EKS_CLUSTER_NAME with the name of your EKS cluster): Creates an IAM role named EKS-GrafanaAgent-AMP-ServiceAccount-Rol e with an IAM policy that has permissions to remote-write into an AMP workspace. Creates a Kubernetes service account named grafana-agent under the grafana-agent namespace that is associated with the IAM role. Creates a trust relationship between the IAM role and the OIDC provider hosted in your Amazon EKS cluster. You need kubectl and eksctl CLI tools to run the gca-permissions.sh script. They must be configured to access your Amazon EKS cluster. Now create a manifest file, grafana-agent.yaml , with the scrape configuration to extract Envoy metrics and deploy the Grafana Agent. Note At time of writing, this solution will not work for EKS on Fargate due to the lack of support for daemon sets there. The example deploys a daemon set named grafana-agent and a deployment named grafana-agent-deployment . The grafana-agent daemon set collects metrics from pods on the cluster and the grafana-agent-deployment deployment collects metrics from services that do not live on the cluster, such as the EKS control plane. kubectl apply -f grafana-agent.yaml After the grafana-agent is deployed, it will collect the metrics and ingest them into the specified AMP workspace. Now deploy a sample application on the EKS cluster and start analyzing the metrics.","title":"Configure permissions"},{"location":"recipes/servicemesh-monitoring-ampamg/#sample-application","text":"To install an application and inject an Envoy container, we use the AppMesh controller for Kubernetes. First, install the base application by cloning the examples repo: git clone https://github.com/aws/aws-app-mesh-examples.git And now apply the resources to your cluster: kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/1_base_application Check the pod status and make sure it is running: $ kubectl -n prod get all NAME READY STATUS RESTARTS AGE pod/dj-cb77484d7-gx9vk 1/1 Running 0 6m8s pod/jazz-v1-6b6b6dd4fc-xxj9s 1/1 Running 0 6m8s pod/metal-v1-584b9ccd88-kj7kf 1/1 Running 0 6m8s Next, install the App Mesh controller and meshify the deployment: kubectl apply -f aws-app-mesh-examples/examples/apps/djapp/2_meshed_application/ kubectl rollout restart deployment -n prod dj jazz-v1 metal-v1 Now we should see two containers running in each pod: $ kubectl -n prod get all NAME READY STATUS RESTARTS AGE dj-7948b69dff-z6djf 2/2 Running 0 57s jazz-v1-7cdc4fc4fc-wzc5d 2/2 Running 0 57s metal-v1-7f499bb988-qtx7k 2/2 Running 0 57s Generate the traffic for 5 mins and we will visualize it AMG later: dj_pod=`kubectl get pod -n prod --no-headers -l app=dj -o jsonpath='{.items[*].metadata.name}'` loop_counter=0 while [ $loop_counter -le 300 ] ; do \\ kubectl exec -n prod -it $dj_pod -c dj \\ -- curl jazz.prod.svc.cluster.local:9080 ; echo ; loop_counter=$[$loop_counter+1] ; \\ done","title":"Sample application"},{"location":"recipes/servicemesh-monitoring-ampamg/#create-an-amg-workspace","text":"To create an AMG workspace follow the steps in the Getting Started with AMG blog post. To grant users access to the dashboard, you must enable AWS SSO. After you create the workspace, you can assign access to the Grafana workspace to an individual user or a user group. By default, the user has a user type of viewer. Change the user type based on the user role. Add the AMP workspace as the data source and then start creating the dashboard. In this example, the user name is grafana-admin and the user type is Admin . Select the required data source. Review the configuration, and then choose Create workspace .","title":"Create an AMG workspace"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-amg-datasource","text":"To configure AMP as a data source in AMG, in the Data sources section, choose Configure in Grafana , which will launch a Grafana workspace in the browser. You can also manually launch the Grafana workspace URL in the browser. As you can see from the screenshots, you can view Envoy metrics like downstream latency, connections, response code, and more. You can use the filters shown to drill down to the envoy metrics of a particular application.","title":"Configure AMG datasource"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-amg-dashboard","text":"After the data source is configured, import a custom dashboard to analyze the Envoy metrics. For this we use a pre-defined dashboard, so choose Import (shown below), and then enter the ID 11022 . This will import the Envoy Global dashboard so you can start analyzing the Envoy metrics.","title":"Configure AMG dashboard"},{"location":"recipes/servicemesh-monitoring-ampamg/#configure-alerts-on-amg","text":"You can configure Grafana alerts when the metric increases beyond the intended threshold. With AMG, you can configure how often the alert must be evaluated in the dashboard and send the notification. Before you create alert rules, you must create a notification channel. In this example, configure Amazon SNS as a notification channel. The SNS topic must be prefixed with grafana for notifications to be successfully published to the topic if you use the defaults, that is, the service-managed permissions . Use the following command to create an SNS topic named grafana-notification : aws sns create-topic --name grafana-notification And subscribe to it via an email address. Make sure you specify the region and Account ID in the below command: aws sns subscribe \\ --topic-arn arn:aws:sns:<region>:<account-id>:grafana-notification \\ --protocol email \\ --notification-endpoint <email-id> Now, add a new notification channel from the Grafana dashboard. Configure the new notification channel named grafana-notification. For Type, use AWS SNS from the drop down. For Topic, use the ARN of the SNS topic you just created. For Auth provider, choose AWS SDK Default. Now configure an alert if downstream latency exceeds five milliseconds in a one-minute period. In the dashboard, choose Downstream latency from the dropdown, and then choose Edit. On the Alert tab of the graph panel, configure how often the alert rule should be evaluated and the conditions that must be met for the alert to change state and initiate its notifications. In the following configuration, an alert is created if the downstream latency exceeds the threshold and notification will be sent through the configured grafana-alert-notification channel to the SNS topic.","title":"Configure alerts on AMG"},{"location":"recipes/servicemesh-monitoring-ampamg/#cleanup","text":"Remove the resources and cluster: kubectl delete all --all eksctl delete cluster --name AMP-EKS-CLUSTER Remove the AMP workspace: aws amp delete-workspace --workspace-id `aws amp list-workspaces --alias prometheus-sample-app --query 'workspaces[0].workspaceId' --output text` Remove the amp-iamproxy-ingest-role IAM role: aws delete-role --role-name amp-iamproxy-ingest-role Remove the AMG workspace by removing it from the console.","title":"Cleanup"},{"location":"recipes/Workspaces-Monitoring-AMP-AMG/","text":"Organizations have started adopting Amazon Workspaces as virtual cloud based desktop as a solution (DAAS) to replace their existing traditional desktop solution to shift the cost and effort of maintaining laptops and desktops to a cloud pay-as-you-go model. Organizations using Amazon Workspaces would need support of these managed services to monitor their workspaces environment for Day 2 operations. A cloud based managed open source monitoring solution such as Amazon Managed Service for Prometheus and Amazon Managed Grafana helps IT teams to quickly setup and operate a monitoring solution to save cost. Monitoring CPU, memory, network, or disk activity from Amazon Workspace eliminates guesswork while troubleshooting Amazon Workspaces environment. A managed monitoring solution on your Amazon Workspaces environments yields following organizational benefits: Service desk staff can quickly identify and drill down to Amazon Workspace issues that need investigation without guesswork by leveraging managed monitoring services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana Service desks staffs can investigate Amazon Workspace issues after the event using the historical data in Amazon Managed Service for Prometheus Eliminates long calls that waste time questioning business users on Amazon Workspaces issues In this blog post, we will set up Amazon Managed Service for Prometheus, Amazon Managed Grafana, and a Prometheus server on Amazon Elastic Compute Cloud (EC2) to provide a monitoring solution for Amazon Workspaces. We will automate the deployment of Prometheus agents on any new Amazon Workspace using Active Directory Group Policy Objects (GPO). Solution Architecture The following diagram demonstrates the solution to monitor your Amazon Workspaces environment using AWS native managed services such as Amazon Managed Service for Prometheus and Amazon Managed Grafana. This solution will deploy a Prometheus server on Amazon Elastic Compute Cloud (EC2) instance which polls prometheus agents on your Amazon Workspace periodically and remote writes metrics to Amazon Managed Service for Prometheus. We will be using Amazon Managed Grafana to query and visualize metrics on your Amazon Workspaces infrastructure.","title":"Index"}]}